---
prev:
    link: ../theme-01
    text: Тема 1. Погрешности
next:
    link: ../theme-03
    text: Тема 3. Точные методы решения СЛАУ
---

**Тема 2**

# Приближённые методы решения СЛАУ



## § 4. Векторы в евклидовом пространстве {#4}

Рассмотрим пространство конечномерных векторов (обозначим их размерность за $n$), в котором определена норма — функция $||\bar{x}||$, заданная на элементах этого пространства, обладающая следующими свойствами:

1. $\left( ||\bar{x}|| \ge 0, ~ ~ ||\bar{x}|| = 0 \right) \iff ( \bar{x} = \bar{0} )$;
2. $||C\bar{x}|| = |C| \cdot ||\bar{x}||$, где $C$ — константа;
3. $||\bar{x} + \bar{y}|| \le ||\bar{x}|| + ||\bar{y}||$.

Наиболее распространёнными являются следующие три нормы:

1. максимум модуля: $||\bar{x}||_1 = \max_{\forall i} \set{ |x_i| }$;
2. сумма модулей всех элементов: $||\bar{x}||_2 = \sum_{i=1}^n |x_i|$;
3. эвклидова норма (корень из суммы квадратов): $||\bar{x}||_3 = \sqrt{ \sum_{i=1}^n x_i^2 }$.

Легко убедиться, что эти свойства выполняются для всех трёх норм:
1. $$ \max_{\forall i} \set{ |x_i + y_i| } \le \max_{\forall i} \set{ |x_i| + |y_i| } \le \max_{\forall i} |x_i| + \max_{\forall i} |y_i|; $$
2. $$ \sum_{i=1}^n |x_i + y_i| \le \sum_{i=1}^n (|x_i| + |y_i|) = \sum_{i=1}^n |x_i| + \sum_{i=1}^n |y_i|; $$
3. $$ \sqrt{ \sum_{i=1}^n (x_i+y_i)^2 } \le \sqrt{\sum_{i=1}^n (x_i)^2} + \sqrt{ \sum_{i=1}^n (y_i)^2 }. $$

   *(неравенство Коши-Буняковского)*

Множество конечномерных векторов, на котором определена норма, называется **линейным нормированным пространством**. В таком пространстве определяется метрика (расстояние) между векторами:

$$ \rho(\bar{x}, \bar{y}) = ||\bar{x} - \bar{y}||. $$

Можно доказать, что данная метрика обладает всеми свойствами расстояния.

::: info ОПРЕДЕЛЕНИЕ
Две нормы называются **эквивалентными**, если существуют два положительных числа $l$ и $L$ такие, что
$$ l \cdot ||\bar{x}||_a \le ||\bar{x}||_b \le L \cdot ||\bar{x}||_a $$
или, что эквивалентно,
$$ {1 \over L} \cdot ||\bar{x}||_b \le ||\bar{x}||_a \le {1 \over l} \cdot ||\bar{x}||_b, $$
где $a$ и $b$ обозначают любые две нормы.
:::

Если рассмотреть две эквивалентные нормы и некоторую последовательность векторов $\bar{x}^{(1)}, \bar{x}^{(2)}, ..., \bar{x}^{(n)}, ...$, то для неё справедливо следующее утверждение:

$$ ||\bar{x}^{(k)} - \bar{x}^*||_a \underset{k\to\infty}{=} 0 \iff ||\bar{x}^{(k)} - \bar{x}^*||_b \underset{k\to\infty}{=} 0, $$
т. е. если последовательность сходится к некоторому пределу в одной норме, то она сходится и во второй.

::: info ТЕОРЕМА
В конечномерном линейном нормированном пространстве все нормы эквивалентны.
:::


## § 5. Матрица как оператор в линейном нормированном пространстве. Нормы матрицы {#5}

Рассмотрим линейное нормированное пространство векторов $H_n$ ($\bar{x} \in H_n$, $n$ — размерность векторов). Рассмотрим также квадратную матрицу $A = (a_{ij})_{n \times n}$.

Если рассматривать в этом пространстве $H_n$ два вектора $\bar{x}$ и $\bar{y}$ такие, что $\bar{y} = A\bar{x}$, то можно считать матрицу $A$ оператором в данном пространстве:

$$ A: ~ x \to y. $$

Будем называть оператор $A$ **ограниченным**, если
$$ \exists C>0: ~ ~ \forall \bar{x}\in H_n ~ ~ ~ ||A\bar{x}|| \le C \cdot ||\bar{x}||, \tag{∗} $$
где $C = \text{const}$.

Ограниченный и аддитивный (допускающий сложение и умножение на число) оператор будем называть **линейным**.

::: info ОПРЕДЕЛЕНИЕ
Минимальное значение $C$, для которого справедливо неравенство $(∗)$, будем называть **нормой оператора** (или **нормой матрицы**) и обозначать $||A||$:

$$ ||A\bar{x}|| \le ||A|| \cdot ||\bar{x}||. $$
:::

Можно доказать, что данное определение эквивалентно следующему:

$$ ||A|| = \sup_{||\bar{x}|| = 1} ||A\bar{x}||, $$

т. е.

$$
||A\bar{x}|| \le ||A|| \cdot ||\bar{x}|| \iff ||A|| = \sup_{||\bar{x}||=1} ||A\bar{x}||.
$$

Отсюда следуют свойства норм операторов. Пусть $A$, $B$, $C$ — некие матрицы, тогда:
1. $$C = A + B \implies C\bar{x} = A\bar{x} + B\bar{x} \implies$$
   $$ \implies \sup ||C\bar{x}|| \le \sup \left( ||A\bar{x}|| + ||B\bar{x}|| \right) \le $$
   $$ \le \sup ||A\bar{x}|| + \sup ||B\bar{x}|| \implies ||A + B|| \le ||A|| + ||B||. $$
2. домножение на число: $C = \alpha A \bar{x}$, $\alpha = \text{const}$.
   $$ \sup ||\alpha A \bar{x}|| = \sup (|\alpha| \cdot ||A\bar{x}||) = |\alpha| \cdot \sup ||A\bar{x}|| \implies $$
   $$ \implies ||\alpha A|| = |\alpha| \cdot ||A||. $$
3. перемножение матриц: $C = A \cdot B$.
   $$ C\bar{x} = AB\bar{x}; $$
   $$ ||C\bar{x}|| = || A\underbrace{B\bar{x}}_\text{вектор} || \le ||A|| \cdot ||B\bar{x}|| \le ||A|| \cdot ||B|| \cdot ||\bar{x}|| \implies $$
   $$ \implies ||AB|| \le ||A|| \cdot ||B||. $$

::: info ЗАМЕЧАНИЕ
Из определения нормы оператора $(∗)$ следует, что каждая конкретная норма оператора должна быть согласована с соответствующей нормой вектора.
:::

Как известно, таких норм всего три:
1. $||\bar{x}||_1 = \underset{i}{\max} |x_i|$;
2. $||\bar{x}||_2 = \sum_{i=1}^n |x_i|$;
3. $||\bar{x}||_3 = \sqrt{ \sum_{i=1}^n x_i^2 }$.

Можно доказать, что данные нормы для операторов имеют вид

1. $||A|| _ 1 = \underset{i}{\max} \sum _ {j=1}^n |a _ {ij}|$<br>*(максимальная сумма модулей по строке)*
2. $||A|| _ 2 = \underset{j}{\max} \sum _ {i=1}^n |a _ {ij}|$<br>*(максимальная сумма модулей по столбцу)*
3. $||A|| _ 3 = \sqrt{\lambda _ {\max}}$

   Здесь рассматривается задача на собственные значения: $A^T A \bar{x} = \lambda\bar{x}$.

   Известно, что $A^TA$ имеет только действительные неотрицательные собственные значения: $\lambda _ 1, \lambda _ 2, ..., \lambda _ n \ge 0$. Тогда $\lambda _ {\max}$ — наибольшее из них.

Покажем, что нормы операторов и векторов согласованы.

1. $$||A\bar{x}|| _ 1 = \underset{i}{\max} \left| \sum^n _ {j=1} a _ {ij} x _ j \right| \le \underset{i}{\max} \sum^n _ {j=1} \left( |a _ {ij}| \cdot |x _ j| \right) \le $$
   $$\le \underset{i}{\max} \left( \underset{k}{\max} |x _ k| \cdot \sum^n _ {j=1} |a _ {ij}| \right) = \underset{i}{\max} \sum^n _ {j=1} |a _ {ij}| \cdot \underset{k}{\max} ~ |x _ k| = $$
   $$ = ||A|| _ 1 \cdot ||\bar{x}|| _ 1.$$

2. $$||A\bar{x}|| _ 2 = \sum^n _ {i=1} \left| \sum^n _ {j=1} a _ {ij} x _ j \right| \le \sum^n _ {i=1} \sum^n _ {j=1} \left( |a _ {ij}| \cdot |x _ j|\right) = $$
   $$ = \sum^n _ {j=1} \underbrace{\left(\sum^n _ {i=1} |a _ {ij}|\right)} _ {\text{выбираем} ~ \underset{j}{\max}} \cdot |x _ j| \le \underset{j}{\max} \sum^n _ {i=1} |a _ {ij}| \cdot \sum^n _ {j=1} |x _ j| = ||A|| _ 2 \cdot ||\bar{x}|| _ 2.$$

3. $||A\bar{x}||^2 _ 3 = \underbrace{(A\bar{x}, A\bar{x})} _ {\text{скалярный квадрат}} = (\bar{x}, A^TA\bar{x}).$

   Известно, что система собственных векторов задачи на собственные значения представляет собой ортогональный базис $\bar{x}^{(1)}, \bar{x}^{(2)}, ..., \bar{x}^{(n)}$.

   Любой вектор может быть представлен в виде разложения по базису:
   $$ \bar{x} = c _ 1 \bar{x}^{(1)} + c _ 2 \bar{x}^{(2)} + \dots + c _ n \bar{x}^{(n)}, $$
   где $c _ 1, c _ 2, ..., c _ n$ — координаты вектора $\bar{x}$.

   Тогда
   $$||A\bar{x}||^2 _ 3 = \left( \sum^n _ {k=1} c _ k \bar{x}^{(k)}, \sum^n _ {j=1} A^TA c _ j \bar{x}^{(j)} \right) = $$
   $$ = \left( \sum^n _ {i=k} c _ k \bar{x}^{(k)}, \sum^n _ {j=1} \lambda _ j \bar{x}^{(j)} \right) = \left| (\bar{x}^{(k)}, \bar{x}^{(j)}) = 0 ~ ~ ~ \forall k \ne j \right| = $$
   $$ = \sum^n _ {j=1} \lambda _ j c^2 _ j (\bar{x}^{(j)}, \bar{x}^{(j)}).$$

   Если базис ортонормированный, то это произведение $(\bar{x}^{(j)}, \bar{x}^{(j)}) = 1$, откуда получаем, что
   $$ ||A\bar{x}||^2 _ 3 = \sum^n _ {j=1} \lambda _ j c^2 _ j \le \lambda _ {\max} \underbrace{\sum^n _ {j=1} c^2 _ j} _ {=\sum^n _ {j=1} x^2 _ j} \implies ||A\bar{x}|| _ 3 \le $$
   $$ \le \sqrt{\lambda _ {\max} \sum^n _ {j=1} x^2 _ j} = ||A|| _ 3 \cdot ||\bar{x}|| _ 3. $$

*Замечание.* Последняя норма слишком сложна для вычисления, поэтому вместо неё часто используют оценку
$$ \lambda _ {\max} \le \sum^n _ {j=1} \lambda _ j = \sqrt{\sum^n _ {i=1} \sum^n _ {j=1} a^2 _ {ij}}. $$


## § 6. Последовательность матриц. Теоремы о сходимости матричного ряда {#6}

::: info ОПРЕДЕЛЕНИЕ
Будем считать, что последовательность матриц $A^{(1)}, A^{(2)}, ..., A^{(k)}, ...$ **сходится** к матрице $A$, если
$$ a _ {ij}^{(k)} \underset{k\to\infty}{\longrightarrow} a _ {ij} ~ ~ ~ (\text{или} ~ A^{(k)} \underset{k\to\infty}{\longrightarrow} A). $$
:::

Очевидно, что данное условие равносильно следующему:
$$ || A^{(k)} - A || \underset{k\to\infty}{\longrightarrow} 0. $$

Можно показать, что справедливы следующие теоремы:

::: info ТЕОРЕМА 1
Следующие условия:
* $A^k \to \mathbb{O}$ ($\mathbb{O}$ — нулевая матрица),
* $E + A + A^2 + \dots + A^k + \dots = (E-A)^{-1}$

выполняются тогда и только тогда, когда все собственные значения матрицы $A$ по модулю меньше единицы.
:::

::: info ЗАМЕЧАНИЕ
В этой теореме в качестве последовательности матриц рассматривается степенной ряд
$$ E = A + A^2 + \dots + A^k + \dots, $$
где $A^k = \underbrace{A\cdot A\cdot ... \cdot A} _ {k ~ \text{раз}}$, $E$ — единичная матрица.
:::

::: info ТЕОРЕМА 2
Если норма матрицы $A$
$$ ||A|| \le q \lt 1, $$
то матрица $E-A$ имеет обратную и
$$ (E-A)^{-1} = \sum^\infty _ {k=0} A^k. $$

$(A^0 = E)$
:::

При этом норма обратной матрицы
$$ || (E-A)^{-1} || \le {1 \over 1 - q}. $$

::: details Доказательство теоремы 2
Рассмотрим матричный ряд
$$ E = A + A^2 + \dots + A^k + \dots. \tag{∗} $$

Т. к. норма $||A^k|| \le ||A||^k$, то данный ряд мажорируется числовым рядом.

$$
||E|| + ||A|| + ||A^2|| + \dots + ||A^k|| + \dots \le $$
$$ \le 1 + q + q^2 + \dots + q^k + \dots = {1 \over 1 - q}.
$$

Следовательно, ряд $(∗)$ сходится, откуда следует, что существует некоторая матрица $S$, которая
$$
\exists S = E + A + A^2 + \dots + A^k + \dots
$$
и
$$
||S|| \le {1 \over 1 - q}.
$$

Умножим $S$ на $E-A$ слева:

$$
(E-A)S = (E+A+A^2+\dots) - (A+A^2+A^3+\dots) = E \implies $$
$$\implies S = (E-A)^{-1}E = (E-A)^{-1}. ~ ~ \blacksquare
$$
:::

::: info ТЕОРЕМА 3
Любая норма матрицы больше или равна модуля любого её собственного значения.
:::

::: details Доказательство теоремы 3
Пусть $A\bar{x} = \lambda\bar{x}$. Тогда
$$
\underbrace{||A\bar{x}||} _ {\le ||A|| \cdot ||\bar{x}||} = ||\lambda\bar{x}|| = |\lambda| \cdot ||\bar{x}||
$$
$$
||A|| \cdot ||\bar{x}|| \ge |\lambda| \cdot ||\bar{x}||
$$
$$
||A|| \ge |\lambda|. ~ ~ \blacksquare
$$
:::


## § 7. Приближённые методы решения СЛАУ. Метод простых итераций {#7}

Пусть требуется решить СЛАУ
$$ A\bar{x} = \bar{b}, $$
где $A = (a _ {ij}) _ {n\times n}$.

Все приближённые методы решения данной задачи построены на следующем принципе:
1. Сначала задаётся начальное приближение $\bar{x}^{(0)}$, которое обычно выбирается произвольным.
2. Затем производится последовательность однотипных вычислений по некоторой реккурентной формуле
$$
\bar{x}^{(k+1)} = f\left( k, \bar{x}^{(k)}, \bar{x}^{(k-1)}, ..., \bar{x}^{(k-m)} \right).
$$

Если функция правой части не зависит от $k$, то метод называется **стационарным**.

Если последующее значение зависит только от одного предыдущего, то метод называется **одношаговым** (в противном случае — **многошаговым**).

Каждый шаг вычислений по данной формуле называется **итерацией**.

Если последовательность итераций имеет предел
$$ \bar{x}^{(k)} \underset{k\to\infty}{\longrightarrow} \bar{x}^* $$
такой, что $A\bar{x}^* = \bar{b}$, то говорят, что метод **сходится** (в противном случае — **расходится**).

### Стационарный одношаговый метод — метод простых итераций (МПИ)

Предположим, что от СЛАУ вида $A\bar{x} = \bar{b}$ мы перешли у СЛАУ вида $\bar{x} = G\bar{x} + \bar{F}$.

(Т. е. обе системы имеют одно и то же решение)

Тогда последовательность приближений $\set{\bar{x}^{(k)}}^\infty _ {k=0}$ строится по рекуррентной формуле:
$$
\bar{x}^{(k+1)} = G\bar{x}^{(k)} + \bar{F},
$$
где $\bar{x}^{(0)}$ выбирается произвольно.

Выясним условие сходимости данного метода.

$$
\bar{x}^{(k+1)} = G\bar{x}^{(k)} + \bar{F} = G \left( G\bar{x}^{(k-1)} + \bar{F} \right) + \bar{F} = $$
$$ = G^2\bar{x}^{(k-1)} + G\bar{F} + \bar{F} = G^2 \left( G\bar{x}^{(k-2)} + \bar{F} \right) + G\bar{F} + \bar{F} = $$
$$ = G^3\bar{x}^{(k-2)} + G^2\bar{F} + G\bar{F} + \bar{F} = $$
$$ = G^{k+1}\bar{x}^{(0)} + \left( G^k + G^{k-1} + \dots + G + E \right) \bar{F}.
$$

Как известно, для того чтобы при $k \to \infty$ степенной ряд сходился, необходимо и достаточно, чтобы все собственные значения матрицы $G$ были по модулю меньше 1.

Если это так, то $G^k \to \mathbb{O}$ (т. е. $G^k$ стремится к нулевой матрице), а $(E+G+G^2 + \dots + G^k) \to (E-G)^{-1}$.

Тогда $\bar{x}^{(k+1)} \to \bar{x}^* = (E-G)^{-1}\bar{F}$.

$\bar{x}^*$ и есть решение исходной задачи, т.к. $\bar{x} = G\bar{x} + \bar{F}$ или $\bar{x}(E-G)=\bar{F}$ или $\bar{x} = \bar{x}^* = (E-G)^{-1}\bar{F}$.

*Вывод:* после преобразования СЛАУ $A\bar{x} = \bar{b}$ к виду $\bar{x} = G\bar{x} + \bar{F}$ метод простой итерации сходится тогда и только тогда, когда $\left| \lambda _ G \right| < 1$.

Как правило, используют достаточное условие сходимости ($||G|| < 1$).

Таким образом, для того чтобы применить данный метод, надо преобразовать СЛАУ так, чтобы выполнялись достаточные условия сходимости.

Исходя из трёх видов нормы матрицы $G=(g _ {ij}) _ {n \times n}$, достаточные условия сходимости могут выглядеть так:
1. $\sum^n _ {j=1} |g _ {ij}| \le \mu \lt 1 ~ ~ ~ ~ \forall i = \overline{1,n}$<br>*(сумма элементов по строкам меньше единицы)*
2. $\sum^n _ {i=1} |g _ {ij}| \le \mu \lt 1 ~ ~ ~ ~ \forall j = \overline{1,n}$<br>*(сумма элементов по столбцам меньше единицы)*
3. $\sum^n _ {i=1} \sum^n _ {j=1} g^2 _ {ij} \le \mu \lt 1$.

::: info ЗАМЕЧАНИЕ
Процесс итерации обычно останавливается, когда достигается наперёд заданная точность $\varepsilon > 0$.
:::

Критерий остановки:
$$ || \bar{x}^{(k+1)} - \bar{x}^{(k)} || \lt \varepsilon $$
или
$$ || A\bar{x}^{(k)} - \bar{b} || \lt \varepsilon. $$

Иногда возникает необходимость предварительной оценки числа итераций.

Если $\bar{x}^*$ — точное решение, то
$$
|| \bar{x}^{(k)} - \bar{x}^* || = || G^{k+1}\bar{x}^{(0)} + (G^k + \dots + G + E)\bar{F} - (E-G)^{-1}\bar{F} || = $$
$$ = || G^{k+1}\bar{x}^{(0)} + (G^k + G^{k-1} + \dots + G + E - E - G - \dots - G^k - \dots)\bar{F} || = $$
$$ = || G^{k+1}\bar{x}^{(0)} - (G^{k+1} + G^{k+2} + \dots)\bar{F} || \le $$
$$ \le ||G||^{k+1} \cdot ||\bar{x}^{(0)}|| + \underbrace{\left( ||G||^{k+1} + ||G||^{k+2} + \dots \right)} _ {\text{геом. прогрессия}} ||\bar{F}|| = $$
$$ = \set{\text{если} ~ ||G| \lt 1} = ||G||^{k+1} \cdot ||\bar{x}^{(0)}|| + { ||G||^{k+1} \over 1 - ||G|| } \cdot ||\bar{F}||.
$$

Обычно берут в качестве начального приближения $\bar{x}^{(0)} = \bar{F}$. Тогда
$$
|| \bar{x}^{(k+1)} - \bar{x}^* || \le {2 - ||G|| \over 1 - ||G||} \cdot ||G||^{k+1} \cdot ||\bar{F}||.
$$

Считая, что правая часть $\lt \varepsilon$, можно определить необходимое число итераций $k$.


### Приведение системы к виду, удобному для итерации

Подразумевается переход СЛАУ от вида $A\bar{x} = \bar{b}$ к виду $\bar{x} = G\bar{x} + \bar{F}$ так, чтобы выполнялось достаточное условие сходимости метода: $||G|| < 1$.

Пусть имеется система вида $A\bar{x} = \bar{b}$. Домножим обе части на $A^T$:

$$ A^T A \bar{x} = A^T \bar{b}. $$

Разделим на норму матрицы (т. е. на число):

$$ { A^T A \over ||A^T A|| }\bar{x} = {A^T \bar{b} \over ||A^T A||}. $$

Перенесём коэффициент в левой части вправо:

$$ \underbrace{E\bar{x} = \bar{x}} _ {\text{слева}} = \underbrace{ \underbrace{\left(E - {A^T A \over ||A^T A||}\right)} _ {G} \bar{x} + \underbrace{A^T \bar{b} \over ||A^T A||} _ {\bar{F}} } _ {\text{справа}}. $$

Рассмотрим задачу на собственные значения:

$$
{A^T A \over ||A^T A||}\bar{x} = \mu \bar{x}, ~ ~ ~ ~ \mu \ge 0.
$$

Возьмём норму от обеих частей:

$$
\underbrace{{||A^T A \bar{x}|| \over ||A^T A||} \le { ||A^T A|| \cdot ||\bar{x}|| \over ||A^T A|| }} _ {\text{слева}} = \underbrace{|\mu| \cdot ||\bar{x}||} _ {\text{справа}};
$$

$$ ||\bar{x}|| \ge |\mu| \cdot ||\bar{x}||; $$

$$ |\mu| \le 1. $$

(т. е. все собственные значения по модулю $\le 1$)

Рассмотрим другую задачу:

$$ \left(E - {A^T A \over ||A^T A||}\right) \bar{x} = \lambda \bar{x} $$

$$ \implies \lambda = 1 - \mu. $$

Если $|A| \ne 0$, то $0 \lt |\mu| \le 1$ $\implies$ $|\lambda| < 1$.

*Вывод:* данное преобразование СЛАУ приводит к выполнению необходимого и достаточного условия сходимости.

### Метод Якоби

Преобразуем задачу следующим образом: было $A\bar{x} = \bar{b}$, а стало $D\bar{x} = (D-A)\bar{x} + \bar{b}$, где $D=(d _ {ij}) _ {n \times n}$ — диагональная матрица , $d _ {ij} = \begin{cases} a _ {ii}, & i = j; \\\\ 0, & i \ne j \end{cases}$.

Отсюда следует, что $\bar{x} = (E - D^{-1}A)\bar{x} + D^{-1}\bar{b}$.

Тогда в развёрнутом виде это будет выглядеть так:

$$ \bar{x}^{(k+1)} = {b _ i \over a _ {ii}} - {1 \over a _ {ii}} \sum _ {j=1, ~ j \ne i}^n a _ {ij} x _ j^{(k)} ~ ~ ~ ~ \forall i = \overline{1,n}, $$

т. е.

$$ E - D^{-1}A = G, $$
$$ D^{-1} \bar{b} = \bar{F}. $$

**Достаточное условие сходимости** определяется тремя видами норм матрицы:

1. $$ \max _ i \sum^n _ {j=1} \left|{a _ {ij} \over a _ {ii}}\right| < 1, ~ ~ ~ j \ne i $$
   или
   $$ {1 \over a _ {ii}} \sum^n _ {j=1} \left|a _ {ij}\right| < 1, ~ ~ ~ j \ne i $$
   или
   $$ \sum^n _ {j=1} |a _ {ij}| \lt |a _ {ii}|, ~ ~ ~ ~ j \ne i. $$

   Последнее условие называется *условием диагонального преобладания*.

   В каждой строке модуль диагонального элемента должен быть больше суммы модулей остальных.

2. $$ \sum^n _ {i=1} \left|{a _ {ij} \over a _ {ii}}\right| < 1 ~ ~ ~ ~ \forall j = \overline{1,n}. $$

3. $$ \sum^n _ {i=1} {1 \over a^2 _ {ii}} \sum^n _ {j=1} c^2 _ {ij} < 1. $$

Можно доказать необходимое и достаточное условия сходимости метода Якоби: он сходится в том и только том случае, когда все корни уравнения

$$ \left|\begin{matrix} a _ {11} & a _ {12} & \dots & a _ {1n} \\\\ a _ {21} & a _ {22} & \dots & a _ {2n} \\\\ \vdots & \vdots & \ddots & \vdots \\\\ a _ {n1} & a _ {n2} & \dots & a _ {nn} \end{matrix}\right| = G $$

по модулю меньше единицы.

Действительно, все собственные значения матрицы $G = E - D^{-1}A$ должны быть меньше 1, но уравнение для них имеет вид

$$ |E - D^{-1}A - \lambda E| = 0. $$

Домножим на $-|D|$:

$$ |\underbrace{(\lambda - 1)} _ {\mu}D + A| = 0. $$

$$ \mu = \lambda - 1 \implies |\mu| = |\lambda - 1| < 1. $$


## § 8. Метод Зейделя {#8}

Перейдём от уравнения $A\bar{x} = \bar{b}$ к разложению $A = C + D + H$, где

* $D = (d _ {ij})$, $d _ {ij} = \begin{cases} a _ {ii}, & i = j, \\\\ 0, & i \ne j; \end{cases}$
* $C = (c _ {ij})$ — верхняя треугольная матрица, $c _ {ij} = \begin{cases} a _ {ij}, & i < j, \\\\ 0, & i \ge j; \end{cases}$
* $H = (h _ {ij})$ — нижняя треугольная матрица, $h _ {ij} = \begin{cases} a _ {ij}, & i > j, \\\\ 0, & i \le j. \end{cases}$

$$ (D+H)\bar{x} = -C\bar{x}+\bar{b}; $$

$$ \bar{x} = \underbrace{-(D+H)^{-1}C} _ {G}\bar{x} + \underbrace{(D+H)^{-1}\bar{b}} _ {\bar{F}}. $$

Отличается от метода простой итерации тем, что найденное приближение для очередной компоненты $x _ i^{(k+1)}$ используется для отыскания следующей компоненты $x^{(k+1)} _ {i+1}$, т. е. в развёрнутом виде получим

$$
x^{(k+1)} _ {i+1} = \underbrace{-\sum^{i-1} _ {j=1} {a _ {ij} \over a _ {ii}}x^{(k)} _ j} _ {(k+1)\text{-й шаг}} \underbrace{- \sum^n _ {j = i + 1} {a _ {ij} \over a _ {ii}} x^{(k)} _ j + {b _ i \over a _ {ii}}} _ {k\text{-й шаг}}. \tag{∗}
$$

Этот метод можно рассматривать как метод простых итераций, если преобразовать его схему в виде

$$
\bar{x}^{(k+1)} = -(D+H)^{-1}C\bar{x}^{(k)} + (D+H)^{-1} \bar{b}.
$$

Тогда необходимым и достаточным условием его сходимости будет требование, чтобы собственные значения матрицы $-(D+H)^{-1}C$ были по модулю меньше единицы, т. е. быть по модулю должны быть корни характеристического уравнения

$$ |-(D+H)^{-1}C - \lambda E = 0| $$

или

$$ |C + \lambda(D+H)| = 0 $$

или

$$ \left|\begin{matrix} a _ {11}\lambda & a _ {12} & \dots & a _ {1n} \\\\ a _ {21}\lambda & a _ {22} & \dots & a _ {2n}\lambda \\\\ \vdots & \vdots & \ddots & \vdots \\\\ a _ {n1}\lambda & a _ {n2}\lambda & \dots & a _ {nn}\lambda \end{matrix}\right| = 0. $$

Это уравнение называется **необходимым и достаточным условием метода Зейделя**.

::: info ЗАМЕЧАНИЕ
Проверять последнее условие неудобно, поэтому воспользуемся достаточными признаками.
:::

::: info ТЕОРЕМА 1
Если в матрице $A$ исходной системы имеет место диагональное преобладание, то метод Зейделя сходится, причём быстрее, чем метод Якоби.
:::

::: info ОПРЕДЕЛЕНИЕ
Система $A\bar{x} + \bar{b}$ называется **нормальной**, если матрица $A$ симметрично и положительно определённая.
:::

::: info ТЕОРЕМА 2
Если система $A\bar{x} + \bar{b}$ нормальная, то метод Зейделя сходится.

*Замечание.* Систему $A\bar{x} + \bar{b}$ можно привести к нормальному виду, домножив на $A^T$:
$$ A^TA\bar{x} = A^T\bar{b} $$
(это называется *симметризацией Гаусса*)
:::

::: info ТЕОРЕМА 3
Пусть $|A| \ne 0$. Тогда система  $A^TA\bar{x} + A^T\bar{b}$ — нормальная.

Таким образом, чтобы метод Зейделя сходился, достаточно привести систему к симметричному виду.
:::


## § 9. Число обусловленности матрицы. Погрешности решений СЛАУ. Две теоремы о погрешностях {#9}

Предположим, что в исходные данные (матрицу и свободные члены СЛАУ) внесено некоторое возмущение (вместо $A$ имеем $A + \delta A$, вместо $\bar{b}$ имеем $\bar{b} + \delta \bar{b}$ и, следовательно, вместо $\bar{x}$ имеем $\bar{x} + \delta \bar{x}$).

В качестве относительной погрешности решения будем рассматривать величину ${ ||\delta \bar{x}|| \over ||\bar{x}|| }$.

:::: info ТЕОРЕМА 1
Пусть $\delta A = 0$. Тогда для возмущённой задачи
$$
A(\bar{x} + \delta \bar{x}) = \bar{b} + \delta \bar{b} \tag{∗}
$$
справедливо неравенство
$$
{ ||\delta \bar{x}|| \over ||\bar{x}|| } \le ||A|| \cdot ||A^{-1}|| \cdot { ||\delta \bar{b}|| \over ||\bar{b}|| }.
$$

::: details Доказательство
Раскроем скобки в системе $(∗)$, учитывая, что $A\bar{x} = \bar{b}$. Получим $A\delta \bar{x} = \delta \bar{b}$, т. е. $\delta \bar{x} = A^{-1} \delta \bar{b}$.

Далее перейдём к нормам:
$$
||\delta\bar{x}|| = ||A^{-1} \delta \bar{b}|| \le ||A^{-1}|| \cdot ||\delta \bar{b}||.
$$

Т. к. $\bar{b} = A\bar{x}$, то
$$
||\bar{b}|| = ||A\bar{x}|| \le ||A|| \cdot ||\bar{x}||.
$$

Умножим одно неравенство на другое и получим требуемый результат:
$$
{1 \over ||\bar{x}||} \le { ||A|| \over ||\bar{b}|| }. ~ ~ ~ ~ \blacksquare
$$
:::

::::

:::: info ТЕОРЕМА 2
Пусть $\delta \bar{b} = 0$. Тогда для возмущённой системы
$$ (A + \delta A)(\bar{x} + \delta \bar{x}) = \bar{b} \tag{∗∗} $$
справедливо неравенство

$$
{ ||\delta \bar{x}|| \over ||\bar{x} + \delta \bar{x}|| } \le ||A|| \cdot ||A^{-1}|| \cdot { ||\delta A|| \over ||A|| },
$$
где первая дробь — это отношение погрешностей решения, а вторая дробь — относительная погрешность исходных данных.

::: details Доказательство
Раскроем скобки в $(∗∗)$, учитывая, что $A\bar{x} = \bar{b}$. Получим
$$
A \delta \bar{x} = -\delta A (\bar{x} + \delta \bar{x}).
$$

Умножим на $-A^{-1}$:
$$
-\delta \bar{x} = A^{-1} \cdot \delta \cdot A \cdot (\bar{x} + \delta \bar{x}).
$$

Перейдём к нормам:
$$
||\delta\bar{x}|| = ||A^{-1} \delta A (\bar{x} + \delta \bar{x})|| \le
$$
$$
\le ||A^{-1}|| \cdot ||\delta A|| \cdot ||\bar{x} + \delta \bar{x}||.
$$

Делим и умножаем неравенство на $||A||$, получаем утверждение теоремы. $\blacksquare$
:::

::::

<br>

::: info ОПРЕДЕЛЕНИЕ
Коэффициент пропорциональности между погрешностями результатов и исходных данных, присутствующий в обоих результатах, называется **числом обусловленности:**
$$
\text{cond} ~ A = ||A^{-1}|| \cdot ||A||.
$$
:::

Свойства числа обусловленности:

1. Очевидно, что выражение для числа обусловленности связано с вычислением нормы матрицы.

   В случае с симметричной матрицей
   $$
   ||A||_3 = \underset{i}{\max} |\lambda_i| \implies ||A^{-1}||_3 = \underset{i}{\max} \left|{ 1 \over \lambda_i }\right| = {1 \over \underset{i}{\min} |\lambda_i|} \implies
   $$
   $$
   \implies \text{cond} ~ A = { \underset{i}{\max} |\lambda_i| \over \underset{i}{\min} |\lambda_i| }.
   $$

2. Матрица называется **плохо определённой**, если её определитель близок к нулю. В этом случае можно умножить обе части СЛАУ на достаточно большое число.

   В случае плохой обусловленности это не помогает, т. к.
   $$
   \text{cond} ~ (cA) = ||(cA)^{-1}|| \cdot ||cA|| = $$
   $$ = {1 \over |c|} \cdot ||A^{-1}|| \cdot |c| \cdot ||A|| = \text{cond} ~ A,
   $$
   где $c = \text{const}$.

3. $$
   \text{cond} ~ A = ||A^{-1}|| \cdot ||A|| \ge ||A^{-1} \cdot A|| = ||E|| = 1.
   $$
   Таким образом, все матрицы можно поделить на *плохо обусловленные* и *хорошо обусловленные*.

   Хорошо обусловленные матрицы имеют число обусловленности, близкое к единице. Плохо обусловленные — большое число обусловленности.

4. Число обусловленности от произведения:
   $$
   \text{cond} ~ (AB) = ||(AB)^{-1}|| \cdot ||AB|| = ||B^{-1} \cdot A^{-1}|| \cdot ||AB|| \le
   $$
   $$
   \le ||B^{-1}|| \cdot ||A^{-1}|| \cdot ||A|| \cdot ||B|| = \text{cond} ~ A \cdot \text{cond} ~ B.
   $$

   Таким образом, поскольку число обусловленности матрицы всегда не меньше единицы, то умножением матрицы на другую его не улучшить.


::: info ПРИМЕР
Пусть требуется решить систему

$$\begin{cases}
x_1 + 0,99x_2 = 1,99; \\
0,99x_1 + 0,98x_2 = 1,97.
\end{cases}$$

Очевидно, что решением является $(1; 1)$.

Внесём возмущение в правую часть:

$$\begin{cases}
x_1 + 0,99x_2 = 1,98; \\
0,99x_1 + 0,98x_2 = 1,96.
\end{cases}$$

В данном случае решением будет $(0; 2)$.

Получается, что
$$
\delta \bar{b} = \left(\begin{matrix} -0,01 \\ -0,01 \end{matrix}\right),
~ ~ ~ ~
\bar{b} \approx \left(\begin{matrix} 2 \\ 2 \end{matrix}\right),
$$
$$
{ \delta \bar{b} \over ||\bar{b}|| } = -\left(\begin{matrix} 0,005 \\ 0,005 \end{matrix}\right) \approx 0,5\%.
$$

Что касается $\bar{x}$, то

$$
\delta \bar{x} = \left(\begin{matrix} 1 \\ 1 \end{matrix}\right),
$$
$$
{ \delta \bar{x} \over ||\bar{x}|| } = \left(\begin{matrix} -1 \\ 1 \end{matrix}\right) \approx 100\%.
$$

(вроде ничтожное изменение исходных данных может привести к сильному изменению решения)

Найдём обратную матрицу:

$$
A^{-1} = \left(\begin{matrix} -9800 & 9900 \\ 9900 & -10000 \end{matrix}\right).
$$

Её число обусловленности:
$$
\text{cond} ~ A = ||A^{-1}||_1 \cdot ||A||_1 = 19900 \cdot 1,99 \approx 40000.
$$

или

$$\begin{cases}
x_1 + 0,99x_2 = 2; \\
0,99x_1 + 0,98x_2 = 1,98.
\end{cases}$$

Очевидно, что решением является $(2; 0)$.
:::


## § 10. Матрицы специального вида {#10}

### Диагональная матрица

**Диагональная матрица** — это матрица $D$ размера $n \times n$ вида

$$
D = \left(\begin{matrix}
d_{11} & 0 & \dots & 0 \\
0 & d_{22} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_{nn}
\end{matrix}\right),
$$

т. е. матрица такая, что все её элементы вне главной диагонали равны нулю.

Определитель такой матрицы равен произведению элементов главной диагонали:

$$
|D| = \prod_{i=1}^n d_{ii}.
$$

Элементы обратной матрицы $D^{-1}$ можно найти так:

$$
(D^{-1})_{ij} = \begin{cases}
{1 \over d_{ii}}, & i = j; \\
0, & i \ne j.
\end{cases}
$$

### N- и M-матрицы

**N-матрица** — это матрица размера $n \times n$, имеющая следующий вид:

$$
N_k = \left(\begin{matrix}
1 & \dots & 0 & 0 & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 1 & 0 & 0 & \dots & 0 \\
0 & \dots & 0 & n_{kk} & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & n_{k+1,k} & 1 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & n_{nk} & 0 & \dots & 1
\end{matrix}\right),
$$

посередине показан $k$-й столбец.

**M-матрица** — это матрица размера $n \times n$, имеющая следующий вид:

$$
M_k = \left(\begin{matrix}
1 & \dots & 0 & 0 & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 1 & 0 & 0 & \dots & 0 \\
0 & \dots & 0 & m_{kk} & m_{k,k+1} & \dots & m_{kn} \\
0 & \dots & 0 & 0 & 1 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & 0 & 0 & \dots & 1
\end{matrix}\right),
$$

посередине показана $k$-ая строка.

Можно легко найти обратные матрицы $N^{-1}$ и $M^{-1}$:

$$
N_k = \left(\begin{matrix}
1 & \dots & 0 & 0 & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 1 & 0 & 0 & \dots & 0 \\
0 & \dots & 0 & {1 \over n_{kk}} & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & -{n_{k+1,k} \over n_{kk}} & 1 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & -{n_{nk} \over n_{kk}} & 0 & \dots & 1
\end{matrix}\right);
$$

$$
M_k = \left(\begin{matrix}
1 & \dots & 0 & 0 & 0 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 1 & 0 & 0 & \dots & 0 \\
0 & \dots & 0 & {1 \over m_{kk}} & -{m_{k,k+1} \over m_{kk}} & \dots & -{m_{kn} \over m_{kk}} \\
0 & \dots & 0 & 0 & 1 & \dots & 0 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \dots & 0 & 0 & 0 & \dots & 1
\end{matrix}\right).
$$

### Треугольные матрицы

Треугольные матрицы делятся на *нижние треугольные* и *верхние треугольные*.

#### Нижняя треугольная матрица

**Нижняя треугольная матрица** — это матрица размера $n\times n$ вида

$$
A = \left(\begin{matrix}
a_{11} & 0 & 0 & \dots & 0 \\
a_{21} & a_{22} & 0 & \dots & 0 \\
a_{31} & a_{32} & a_{33} & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & a_{n3} & \dots & a_{nn}
\end{matrix}\right).
$$

Определитель $|A|$ такой матрицы равен произведению элементов главной диагонали:

$$
|A| = \prod_{i=1}^n a_{ii}.
$$

Такую матрицу можно разложить на произведение N-матриц:

$$
A = N_1 N_2 \dots N_n,
$$

$$
A = N^{-1}_n N^{-1}_{n-1} \dots N^{-1}_1.
$$

#### Верхняя треугольная матрица

**Верхняя треугольная матрица** — это матрица размера $n\times n$ вида

$$
A = \left(\begin{matrix}
a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
0 & a_{22} & a_{23} & \dots & a_{2n} \\
0 & 0 & a_{33} & \dots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & a_{nn}
\end{matrix}\right).
$$

Такую матрицу можно разложить на произведение M-матриц:

$$
A = M_nM_{n-1}\dots M_1,
$$

$$
A = M^{-1}_1 M^{-1}_2 \dots M^{-1}_n.
$$

Ненулевые элементы $k$-го столбца в матрице $N_k$ ($k$-ой строки в матрице M_k$) совпадают с соответствующими элементами столбца (строки) матрицы $A$.

Таким образом, система
$$A\bar{x} = \bar{b}$$
с треугольной матрицей легко решается в виде
$$ \bar{x} = A^{-1}\bar{b}, $$
где обратная матрица раскладывается в произведение N- или M-матриц.

### Матрица перестановок

Получается из единичной перестановкой $i$ ($j$-ой) строк (столбцов).

![](/media/images/num%20theory%2010%201.jpg)

$$ P_{ij} = P_{ij}^{-1} $$
$$ |P_{ij}| = -1. $$

Умножение матрицы $A$ на матрицу перестановок $P_{ij}$ справа приводит к перестановке столбцов с номерами $i$ и $j$, а умножение слева — к перестановке строк с теми же номерами.

Умножение матрицы $A$ на матрицу перестановок $P_{ij}$ справа приводит к перестановке с номерами $ij$.

Произведение нескольких матриц перестановок даёт *перестановочную матрицу*, в которой в каждом столбце и каждой строке — по одной единице, а все остальные — нули.

### Матрицы с ортогональными столбцами

Столбцы $i$ и $j$ *ортогональны*, если $\sum_{k=1}^n a_{ki}a_{kj} = 0$.

Пусть $Q$ — матрица с ортогональными столбцами. Но тогда $Q^T \cdot Q = D$ (диагональная).

$$ (Q^T Q)^{-1} = D^{-1}; $$
$$ Q^{-1} (Q^T)^{-1} = D^{-1}; $$
$$ \boxed{ ~ Q^{-1} = D^{-1} \cdot Q^T ~ } $$

Таким образом, обращение матрицы сводится к её транспонированию.

#### Матрица простейшего поворота

Отличается от единичной $i$-й и $j$-й строками и столбцами.

![](/media/images/num%20theory%2010%202.jpg)

$$ |T_{ij}| = 1 \implies (T_{ij})^{-1} = T^T. $$

Данная матрица определяет плоский поворот в $n$-мерном пространстве на угол $\alpha$ по измерениям $i$ и $j$.

Произведение нескольких матриц элементарных поворотов задаёт *матрицу вращения*.

#### Матрица отражений

Пусть $\bar{\omega}$ — вектор единичной нормали ($|\bar{\omega}| = 1$) к некоторой плоскости в $n$-мерном пространстве.


![](/media/images/num%20theory%2010%203.jpg){width=320}

Рассмотрим матрицу $\Omega_{n \times n}$:

$$ (\Omega)_{ij} = (\omega_i \cdot \omega_j) $$

или

$$
\Omega = \bar{\omega} \cdot \bar{\omega}^T \implies \Omega^T = \Omega
$$

(матрица симметричная).

Матрицу $U = E - 2\Omega$ определяет отражение вектора на плоскости, заданное единичной нормалью.

Будем использовать элементарные матрицы отображения.

![](/media/images/num%20theory%2010%204.jpg)

$$ |U_i| = -1. $$