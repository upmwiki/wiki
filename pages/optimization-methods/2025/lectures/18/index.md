---
prev: false
next: false
---

**Тема 3. Задачи нелинейного программирования**

# § 18. Классические методы оптимизации

**Задачей нелинейного (математического) программирования** (ЗНП) будем называть задачи следующего вида:

$$
\begin{cases}
f(\bar{x}) = f(x_1, ..., x_n) \to \max ~ (\min) \\
\varphi_1(\bar{x}) = 0, \\
\cdots \\
\varphi_k(\bar{x}) = 0, \\
\varphi_{k + 1}(\bar{x}) \le 0, \\
\cdots \\
\varphi_m(\bar{x}) \le 0.
\end{cases}
$$

В задаче могут присутствовать ограничения по знаку, обычно они включаются в данную группу неравенств.

Из курса математического анализа известно, что задачи такого вида без ограничений называются **задачами на экстремум**, где **экстремумами** называются точки локального максимума и минимума функции, т. е. такие точки $\bar{x}^*$, которые лежат внутри области $X$, такой, что $f(\bar{x}) \le f(\bar{x}^*)$ или $f(\bar{x}) \ge f(\bar{x}^*)$ для любого $\bar{x} \in X$.

**Необходимым условием экстремума** является требование равенства всех частных производных $f(\bar{x}^*)$ нулю:

$$
f'_{x_i}(\bar{x}^*) = 0, ~ ~ ~ ~ i = \overline{1, n}.
$$

**Достаточное условие экстремума** определяет знак второго дифференциала от $f(\bar{x}^*)$ в стационарной точке:

$$
\text{d}^2 f (\bar{x}^*) = \sum_{i=1}^n \sum_{j=1}^n f''_{x_i x_j} (\bar{x}^*) \Delta x_i \Delta x_j.
$$

Достаточные условия выглядят следующим образом:

1. Функция $f(\bar{x})$ имеет в стационарной точке $x = \bar{x}^*$ максимум, если $\text{d}^2 f(\bar{x}^*) < 0$, и минимум, если $\text{d}^2 f(\bar{x}^*) > 0$, при любых $\Delta x_i$, таких, что $\sum\limits_{i=1}^n \Delta x_i^2 \ne 0$.

2. Если $\text{d}^2 f(\bar{x}^*)$ принимает разные знаки в зависимости от выбора $\Delta x_i$, то в точке $x = \bar{x}^*$ экстремума нет.

Выше шла речь о задаче на локальный экстремум. В случае наличия дополнительных ограничений возникает **задача о глобальном экстремуме** в некоторой области $\bar{x} \in \mathcal{D}$, если неравенство $f(\bar{x}^*) \le f(\bar{x})$ или $f(\bar{x}^*) \ge f(\bar{x})$ выполняется для любой точки $\bar{x} \in \mathcal{D}$. Для данной задачи справедлива *теорема Вейерштрасса*.

::: info Теорема Вейерштрасса
Если область $\mathcal{D}$ замкнута и ограничена, то дифференцируемая функция $f(x)$ достигает в этой области своих наибольшего и наименьшего значений или в стационарной точке или в граничной точке данной области.
:::

Таким образом, для определения наибольшего (наименьшего) значения функции $f(\bar{x})$ в области $\mathcal{D}$ необходимо:

1. Найти все стационарные точки внутри области $\mathcal{D}$ и значения функции в них.
2. Исследовать функцию на экстремум на границе области $\mathcal{D}$.
3. Выбрать наибольшее (наименьшее) значение функции, полученное в пунктах 1 и 2.

Как мы знаем из задач линейного программирования, система ограничений-неравенств образует некоторую область (необязательно выпуклую) в $n$-мерном пространстве. Таким образом, задача нелинейного программирования только с ограничениями-неравенствами может рассматриваться как задача на наибольшее (наименьшее) значение функции в области.

Предположим, что в задаче присутствуют только ограничения-равенства. Такая задача также рассматривалась в курсе математического анализа — она называется **задачей на условный экстремум**. Было показано, что для её решения используется вспомогательная функция — **функция Лагранжа:**

$$
L(\bar{x}) = f(\bar{x}) + \sum_{i=1}^m \lambda_i \varphi_i(\bar{x}). ~ ~ ~ ~ \textcolor{gray}{(k = m)}
$$

Эту функцию следует рассматривать как функцию $n + m$ переменных $x_1, ..., x_n, \lambda_1, ..., \lambda_m$, и её локальный экстремум совпадает с локальными экстремумами функции $f(\bar{x})$.

Таким образом, для поиска локальных экстремумов решаем систему

$$
\begin{cases}
\displaystyle {\partial L \over \partial x_j} = 0, & j = \overline{1, n}, \\
\displaystyle {\partial L \over \partial \lambda_i} = 0, & i = \overline{1, m}.
\end{cases}
$$

Очевидно, что вторая группа уравнений совпадает с ограничениями равенства

$$
\varphi_i(\bar{x}) = 0, ~ ~ ~ ~ i = \overline{1, m}.
\tag{∗}
$$

**Достаточное условие экстремума** определяется знаком второго дифференциала $\text{d}^2 L (x_1^*, ..., x_n^*, \lambda_1^*, ..., \lambda_m^*)$ для каждой стационарной точки. При этом в данном случае второй дифференциал вычисляется с учётом уравнений связи $(∗)$, которые приводят к зависимости между приращениями:

$$
\sum_{j=1}^n {\partial \varphi_i (\bar{x}^*) \over \partial x_j} \Delta x_j = 0, ~ ~ ~ ~ i = \overline{1,m}.
$$