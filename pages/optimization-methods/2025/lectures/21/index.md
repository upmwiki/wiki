---
prev:
    text: § 20. Теорема Куна-Таккера о седловой точке
    link: ../20/
next: false
---

**Тема 3. Задачи нелинейного программирования**

# § 21. Приближённые методы решения ЗВП

Обычно данная задача решается одним из семейства градиентных методов.

Основная идея состоит в том, что выбирается начальное приближение, удовлетворяющее системе ограничений. Затем в данной точке вычисляется градиент целевой функции или какая-ибо его проекция. Далее производится движение в этом направлении с заданным шагом до тех пор, пока либо функция перестанет убывать либо мы не выйдем за границы области $X$. В полученной точке пересчитываем градиент и продолжаем движение. Процесс останавливается, когда будет достигнута заданная наперёд точность.

Все эти методы имеют свои достоинства и недостатки, но наиболее простым по алгоритму является *метод Ньютона-Рафсона*.

## Метод Ньютона-Рафсона

Рассмотрим сначала задачу на безусловный экстремум:

$$
z = f(\bar{x}) \to \min.
$$

Считаем $f(\bar{x})$ дважды непрерывно-дифференцируемой по совокупности аргументов. Заменим $f(\bar{x})$ на её тейлоровское разложение до второго порядка включительно:

$$
z = f(\bar{x}^{(k)})
+ \sum_{i=1}^n f'_{x_i} (\bar{x}^{(k)}) (x_i - x_i^{(k)})
+ {1 \over 2} \sum_{i=1}^n \sum_{j=1}^n f''_{x_i x_j} (x_i - x_i^{(k)}) (x_j - x_j^{(k)}).
$$

Чтобы найтти $\bar{x}^{(k+1)}$, возьём частные производные от данного выражения и приравняем к нулю. В результате получим СЛАУ следующего вида:

$$
G^{(k)} \cdot \Delta \bar{x}^{(k)} = -\nabla f (\bar{x}^{(k)}),
$$

где
* $\Delta \bar{x}^{(k)} = \bar{x}^{(k + 1)} - \bar{x}^{(k)}$;
* $G$ — матрица Гессе (матрица из вторых производных): $G_{ij}^{(k)} = f''_{x_i x_j} (\bar{x}^{(k)})$;
* $\nabla f(\bar{x}^{(k)})$ — градиент: $\nabla f(\bar{x}^{(k)}) = \left( f'_{x_1} (\bar{x}^{(k)}), ..., f'_{x_n} (\bar{x}^{(k)}) \right)^T$.

Отсюда получается, что

$$
\bar{x}^{(k+1)} = \bar{x}^{(k)} - (G^{(k)})^{-1} \cdot \nabla f (\bar{x}^{(k)}).
$$

Преимущество этого метода в том, что размер шага каждый раз вычисляется автоматически.

В случае задачи с ограничениями $\varphi_i(\bar{x}) \le 0$, $i = \overline{1,m}$ можно перейти от задачи на условный экстремум к задаче на безусловный экстремум, используя теорему Куна-Таккера. Например, вместо функции $f(\bar{x})$ использовать функцию

$$
F(\bar{x}, \bar{\lambda})
= \sum_{j=1}^n \left( L_x(\bar{x}, \bar{\lambda}) \right)^2
+ \sum_{i=1}^m \lambda_i^2 \varphi_i^2 (\bar{x}).
$$
Очевидно, что данная функция ограничена снизу, и она должна достигать своего минимального значения $F( \bar{x}^{(0)}, \bar{\lambda}^{(0)} ) = 0$. Это означает, что должны выполняться условия 1) и 2) из теоремы Куна-Таккера. Так как оптимальное решение единственно, то алгоритм должен сойтись к нему.

Вычисление вторых производных для такой функции может привести к сложным выражениям, поэтому можно испольховать их конечноразностные аппроксимации.