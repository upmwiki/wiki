---
prev:
    text: Корреляционный анализ количественных зависимостей
    link: ../03/
next:
    text: Классическая модель множественной линейной регрессии
    link: ../05/
---

**Математические модели в экономике**

# Метод наименьших квадратов

<p class="subtext">Лекция</p>

Метод наименьших квадратов (МНК, *англ.* Ordinary Least Squares, OLS) — один из фундаментальных инструментов прикладной математики, статистики и машинного обучения.

МНК был впервые применён Лежандром (1805) и Гауссом (около 1809) для обработки астрономических данных. Гаусс также показал его связь с нормальным распределением.

## Постановка задачи

Требуется подобрать такие значения коэффициентов $\beta$, чтобы график функции

$$
f(x) = \beta_1 \phi_1(x) + \beta_2 \phi_2(x) + \cdots + \beta_m \phi_m(x)
$$

подходил *как можно ближе* к заданным точкам $(x_i, y_i)$, $i = 1,2,...,n$.

Здесь $\beta_i$ — неизвестные коэффициенты, $\phi_i$ — известные функции.

<figure>
    <img src="/media/images/mme_04_01.png" />
</figure>

Нужно как-то конкретизировать понятие *близости* кривой к точкам. В методе наименьших квадратов используется следующее:

> Для заданных точек $(x_i, y_i)$, $i = 1, 2, ..., n$ найти такие значения $\beta_i$, чтобы минимизировать *остаточную сумму квадратов:*
>
> $$
> ESS = \sum_{i=1}^n (y_i - f(x_i))^2 \to \min
> \tag{∗}
> $$

<figure>
    <img src="/media/images/mme_04_02.png" />
</figure>

## Формулы МНК

Введём обозначения:

$$
Y = \begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix},
~~~~
X = \begin{pmatrix}
\phi_1(x_1) & \phi_2(x_1) & \cdots & \phi_m(x_1) \\
\phi_1(x_2) & \phi_2(x_2) & \cdots & \phi_m(x_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_1(x_n) & \phi_2(x_n) & \cdots & \phi_m(x_n) \\
\end{pmatrix},
$$

$$
\beta = \begin{pmatrix}
\beta_1 \\ \beta_2 \\ \vdots \\ \beta_m
\end{pmatrix}.
$$

Тогда

$$
ESS = (Y - X \beta)^T (Y - X \beta).
$$

Условия минимума:

$$
{\partial ESS \over \partial \beta} = 0;
$$

$$
{\partial^2 ESS \over (\partial \beta)^2} = 2 X^T X > 0.
$$

Отсюда

$$
\beta_\text{MHK} = \hat{\beta} = (X^T X)^{-1} X^T Y,
$$

если соответствующая обратная матрица существует.

:::: info Пример
Имеются следующие экспериментальные данные:

|||
|-|-|
| $X$ | 2 | 3 | 3 | 5 | 7 |
| $Y$ | 8 | 7 | 8 | 8 | 19 |

Для этих данных требуется подобрать наилучшую формулу вида $y = \beta_1 x + \beta_2 \sin x$ методом наименьших квадратов.

::: info Решение
Здесь $\varphi_1(x) = x$, $\varphi_2(x) = \sin x$.

$$
Y = \begin{pmatrix}
    8 \\ 7 \\ 8 \\ 8 \\ 19
\end{pmatrix},
~~~~
X = \begin{pmatrix}
    2 & \sin 2 \approx 0,91 \\
    3 & \sin 3 \approx 0,14 \\
    3 & 0,14 \\
    5 & \sin 5 \approx -0,96 \\
    7 & \sin 7 \approx 0,66
\end{pmatrix}.
$$

$$
X^T X = \begin{pmatrix}
2 & 3 & 3 & 5 & 7 \\
0,91 & 0,14 & 0,14 & -0,96 & 0,66
\end{pmatrix}
\begin{pmatrix}
2 & 0,91 \\
3 & 0,14 \\
3 & 0,14 \\
5 & -0,96 \\
7 & 0,66
\end{pmatrix} \approx
\begin{pmatrix}
    96 & 2,47 \\
    2,47 & 2,22
\end{pmatrix}.
$$

$$
X^T Y = \begin{pmatrix}
    2 & 3 & 3 & 5 & 7 \\
    0,91 & 0,14 & 0,14 & -0,96 & 0,66
\end{pmatrix}
\begin{pmatrix}
    8 \\ 7 \\ 8 \\ 8 \\ 19
\end{pmatrix} \approx
\begin{pmatrix}
    234 \\ 14,2
\end{pmatrix}.
$$

$$
(X^T X)^{-1} \approx \begin{pmatrix}
    0,011 & -0,012 \\
    -0,012 & 0,464
\end{pmatrix}.
$$

$$
\beta = (X^T X)^{-1} X^T Y \approx
\begin{pmatrix}
    0,011 & -0,012 \\
    -0,012 & 0,464
\end{pmatrix}
\begin{pmatrix}
    234 \\ 14,2
\end{pmatrix}
\approx
\begin{pmatrix}
    2,34 \\ 3,8
\end{pmatrix}.
$$

Таким образом, наилучшее МНК-приближение в заданном виде для исходных данных имеет вид $y = 2,33 x + 3,8 \sin x$.

Интересно сравнить экспериментальные и расчётные значения для этой формулы:

|||||||
|-|-|-|-|-|-|
| $X$ | 2 | 3 | 3 | 5 | 7 |
| $Y$ | 8 | 7 | 8 | 8 | 19 |
| $\hat{Y}$ | 8,13 | 7,56 | 7,56 | 8,06 | 18,87 |

$$
ESS = (8 - 8,13)^2 + (7 - 7,56)^2 + \cdots
$$
:::
::::

## Геометрическая интерпретация

$$
Y = \begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix},
~~~~
X = \begin{pmatrix}
\phi_1(x_1) & \phi_2(x_1) & \cdots & \phi_m(x_1) \\
\phi_1(x_2) & \phi_2(x_2) & \cdots & \phi_m(x_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_1(x_n) & \phi_2(x_n) & \cdots & \phi_m(x_n) \\
\end{pmatrix},
$$

$$
\beta = \begin{pmatrix}
\beta_1 \\ \beta_2 \\ \vdots \\ \beta_m
\end{pmatrix}.
$$

* Вектор $Y \in \mathbb{R}^n$.
* Пространство столбцов $X$ — это подпространство размерности $m + 1$.
* МНК ищет *ближайший к вектору $Y$ вектор $\hat{Y}$ в этом подпространстве*.

<figure>
    <img src="/media/images/mme_04_03.png" />
</figure>

Ясно, что этот *вектор прогнозных значений*

$$
\hat{Y} = X \hat{\beta}
$$

должен быть *ортогональной проекцией* вектора $Y$ на это пространство столбцов:

$$
\hat{Y} = PY,
~~~~
P = X(X^T X)^{-1} X^T.
$$

<figure>
    <img src="/media/images/mme_04_04.png" />
</figure>

Соответственно, *вектор остатков*

$$
e = \hat{\varepsilon} = Y - \hat{Y} = (I - P) Y
$$

*ортогонален* гиперплоскости, образованной столбцами матрицы $X$:

<figure>
    <img src="/media/images/mme_04_05.png" />
</figure>

## Достоинства и недостатки МНК

Метод наименьших квадратов нахождения статистических оценок обладает многими достоинствами, например:

* простота вычислений;
* единственность решения;
* лёгкость математического анализа получаемых формул.

Но у него есть и недостатки. Главный — из-за возведения в квадрат результатов может сильно зависеть от "выбросов", "резко выделяющихся наблюдений". Устойчивость к такого рода помхам называется *робастностью* метода.

## Другие методы оценивания

### Метод наименьших модулей

Здесь минимизируется не сумма квадратов остатков, а сумма их *модулей:*

$$
\sum_{i=1}^n |e_i| = \sum_{i=1}^n |y_i - f(x_i)| \to \min
$$

Метод наименьших модулей обладает наибольшей устойчивостью к выбросам. Недостатки его:

* сложность вычислений по сравнению с МНК;
* отсутствие единственности решения;
* сложность анализа получаемых формул.

МНМ применяется в ситуациях, где данные содержат выбросы, не нормальны или имеют тяжёлые хвосты в распределении ошибок.

1. **Регрессионный арализ с выбросам:**

   *Пример:* в экономике, при анализе зависимости расходов на рекламу ($x$) от продаж ($y$), несколько аномально высоких значений продаж (например, из-за сезонных акций) могут исказить МНК. МНМ даёт более устойчивую модель.

2. **Обработка данных с не нормальным законом распределения:**

   Когда ошибки в данных следуют распределению с тяжёлыми хвостами (например, Лапласа), МНМ предпочтительнее, так как он соответствует медианной регрессии (ошибки минимизируются по медиане, а не среднему).

   *Пример:* в медицинских исследованиях, где показатели (например, уровень сахара в крови) могут иметь не нормальное распределение, МНМ помогает построить регрессию зависимости от возраста или диеты.

3. **Машинное обучение и анализ данных:**

   МНМ используется в робастной регрессии и как часть алгоритмов машинного обучения (например, в квантильной регрессии).

   *Пример:* прогозирование цен на недвижимость, где аномально дорогие объекты (например, элитные особняки) не должны искажать модель.

4. **Геодезия и картография:**

   МНМ используется для выравнивания геодезических сетей, где измерения (например, углы или расстояния) могут содержать грубые ошибки.

5. **Экономика и финансы:**

   МНМ применяется для моделирования экономических данных с аномалиями, таких как резкие скачки цен или доходности.

   *Пример:* построение модели зависимости цены актива от рыночного индекса, где внезапные кризисы создают выбросы.

6. **Обработка сигналов и изображений:**

   МНМ помогает сглаживать сигналы с шумом, где выбросы (например, импульсный шум) встречаются часто.

### Метод максимального правдоподобия

Метод максимального правдоподобия — это статистический метод, используемый для оценки параметров модели, которые с наибольшей вероятностью объясняют наблюдаемые данные. ММП основан на предположении, что данные порождены некоторым вероятностным распределением с неизвестными параметрами $\theta$.

Цель — найти значения $\theta$, которые максимизируют функцию правдоподобия, то есть вероятность получения наблюдаемых данных при заданных параметрах.

Это в некотором смысле более общий метод, чем предыдущие.

Пусть данные $(z_1, z_2, ...)$ порождены случайным распределением с плотностью $f(x, \theta)$, зависящей от неизвестного параметра $\theta$ (или параметров). Требуется по выборке найти наиболее "удачную" приближённую оценку для $\theta$.

В рамках предыдущих рассмотрений в качестве $z$ берутся остатки $e_i = y_i - f(x_i)$.

<!-- Полагаю, что-то ещё тут было?.. -->

:::: info Пример
Случайная величина $X$ распределена по закону с плотностью распределения:

$$
f(x) = {\lambda \over 2} \cdot e^{-\lambda |x|}.
$$

Требуется *методом максимального правдоподобия* найти оценку для параметра $\lambda$ по выборке

|||||||
|-|-|-|-|-|-|
| $X$ | -2,5 | -1 | 1,2 | 2,6 | 5 |
| $n$ | 2 | 3 | 3 | 1 | 1 |

::: info Решение
Функция правдоподобия $L$ равна произведению плотностей $f(x)$ для нашей выборки:

$$
L = \left( {\lambda \over 2} \cdot e^{-\lambda \cdot |-2,5|} \right) \cdot \left( {\lambda \over 2} \cdot e^{-\lambda \cdot |-1|} \right) \cdot ...
= \text{const} \cdot \lambda^{19,2} e^{-12,3 \lambda}.
$$

Надо найти *максимум* этой функции, или, что обычно проще, *максимум её логарифма:*

$$
\ln L = 19,2 \cdot \ln \lambda - 12,3 \cdot \lambda + \text{const};
$$

$$
(\ln L)' = {19,2 \over \lambda} - 12,3 = 0;
$$

$$
\hat{\lambda} = {19,2 \over 12,3} \approx 1,56.
$$

В последней строке записана искомая оценка параметра $\lambda$, найденная по ММП.
:::
::::

:::: info Решение
Пусть известно, что случайный фактор $\varepsilon$ в регрессии имеет *нормальный* закон распределения $N(0; 5)$. Найти оценки параметров регрессии при помощи ММП.

| $y$ | $x_1$ | $x_2$ |
|-|-|-|
| 13,79 | 1 | 4 |
| 16,61 | 2 | 4 |
| 8,30 | 2 | 3 |
| 16,07 | 3 | 4 |
| 29,12 | 4 | 5 |
| 27,99 | 4 | 4 |
| 25,79 | 5 | 6 |
| 29,69 | 4 | 5 |
| 26,37 | 5 | 3 |
| 22,63 | 6 | 2 |

::: info Решение
Рассмотрим уравнение регрессии:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon.
$$

Плотность распределения для $N(0;5)$ имеет вид

$$
f(x) = {1 \over \sqrt{2\pi \cdot 5}} \exp\left( -{x^2 \over 2 \cdot 5} \right) \implies
$$

$$
\implies \ln f(x) = -{x^2 \over 2 \cdot 5} - \ln(\sqrt{2\pi \cdot 5}).
$$

$$
\ln L = \sum \ln f(e_i) = \text{const} - {\sum e_i^2 \over 2 \cdot 5} = \text{const} - {\sum e_i^2 \over 10} \to \max,
$$

$$
e = Y - (\beta_0 + \beta_1 X_1 + \beta_2 X_2).
$$

Максимизация выражения $\text{const} - {\sum e_i^2 \over 10}$ эквивалентна минимизации суммы квадратов в правой части,

$$
ESS = \sum e_i^2 \to \min,
$$

откуда видно, что в случае нормального закона всё сводится к МНК.
:::
::::