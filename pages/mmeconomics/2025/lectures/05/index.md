---
prev:
    text: Метод наименьших квадратов
    link: ../04/
next:
    text: Нелинейные модели
    link: ../06/
---

**Математические модели в экономике**

# Классическая модель множественной линейной регрессии

<p class="subtext">23 сентября 2025 · лекция</p>

Основным инструментом эконометрики является модель **множественной линейной регрессии:**

$$
Y = \beta_0 + \beta_1 x_1 + \cdots + \beta_m x_m + \varepsilon,
$$

где

* $Y$ — зависимая переменная;
* $x_1, ..., x_m$ — объясняющие переменные (также называемые *регрессорами*),
* $\beta_0, \beta_1, ..., \beta_m$ — коэффициенты регрессии,
* $\varepsilon$ — случайный член.

> Количество неизвестных коэффициентов равно $m + 1$.

Для нахождения оценок коэффициентов $\beta_0, \beta_1, ..., \beta_m$ проводятся $n$ наблюдений ($n > m$), в результате чего получается $n$ равенств, которые удобно записать сразу в матричной форме:

$$
Y = X \beta + \varepsilon,
$$

где

$$
Y = \begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix},
~~~~
X = \begin{pmatrix}
1 & x_{11} & \cdots & x_{m1} \\
1 & x_{12} & \cdots & x_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{1n} & \cdots & x_{mn}
\end{pmatrix},
$$

$$
\beta = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_m
\end{pmatrix},
~~~~
\varepsilon = \begin{pmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_m
\end{pmatrix}.
$$

## Формула МНК

Будем искать оценки $\hat{\beta}$ коэффициентов $\beta$ *методом наименьших квадратов:*

$$
ESS = \sum (y_i - \hat{y}_i)^2 \to \min;
$$

$$
~ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1i} + \hat{\beta}_2 x_{2i} + \cdots ~
$$

Отсюда

$$
\hat{\beta} = \left( X^T X \right)^{-1} X^T Y
$$

:::: info Пример 1
В таблице представлены объём импорта ($Y$), ВНП ($X_1$) и индекс потребительских цен ($X_2$) в США.

| Годы | 1964 | 1965 | 1966 | 1967 | 1968 |
|-|-|-|-|-|-|
| $X_1$ | 636 | 689 | 753 | 796 | 868 |
| $X_2$ | 93 | 95 | 97 | 100 | 104 |
| $Y$ | 28 | 32 | 38 | 41 | 53 |

Требуется построить *оценки параметров уравнения линейной регрессии $Y$ на $X_1$, $X_2$:*

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon.
$$

::: info Решение
Запишем матрицы:

$$
Y = \begin{pmatrix}
28 \\ 32 \\ 38 \\ 41 \\ 53
\end{pmatrix},
~~~~
X = \begin{pmatrix}
1 & 636 & 93 \\
1 & 689 & 95 \\
1 & 753 & 97 \\
1 & 796 & 100 \\
1 & 868 & 104
\end{pmatrix},
~~~~
\hat{\beta} = \begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \\
\hat{\beta}_2
\end{pmatrix}.
$$

Тогда

$$
X^T X
= 
\begin{pmatrix}
1 & 1 & 1 & 1 & 1 \\
636 & 689 & 753 & 796 & 868 \\
93 & 95 & 97 & 100 & 104
\end{pmatrix}
\begin{pmatrix}
1 & 636 & 93 \\
1 & 689 & 95 \\
1 & 753 & 97 \\
1 & 796 & 100 \\
1 & 868 & 104
\end{pmatrix}
=
$$
$$
= 
\begin{pmatrix}
5 & 3742 & 489 \\
3742 & 2833266 & 367516 \\
489 & 367516 & 47899
\end{pmatrix};
$$

$$
X^T Y
=
\begin{pmatrix}
1 & 1 & 1 & 1 & 1 \\
636 & 689 & 753 & 796 & 868 \\
93 & 95 & 97 & 100 & 104
\end{pmatrix}
\begin{pmatrix}
28 \\ 32 \\ 38 \\ 41 \\ 53
\end{pmatrix}
=
\begin{pmatrix}
192 \\ 147110 \\ 18942
\end{pmatrix};
$$

$$
(X^T X)^{-1} =
\begin{pmatrix}
2452,8 & 1,822 & -39,02 \\
1,822 & 0,0014 & -0,03 \\
-39,02 & -0,03 & 0,625
\end{pmatrix};
$$

$$
\hat{\beta} = (X^T X)^{-1} X^T Y = 
\begin{pmatrix}
-150,99 \\ 0,02 \\ 1,78
\end{pmatrix}.
$$

$$
y = -150,99 + 0,02 x_1 + 1,78 x_2.
$$
:::
::::

## Использование Excel

Для нахождения параметров регрессии при помощи MS Excel необходимо, чтобы была установлена надстройка «Анализ данных».

::: info Пример
Возьмём тот же пример и внесём данные в таблицу MS Excel *по столбцам*:

<figure>
    <img src="/media/images/mme_05_01.png" />
</figure>

Выберем пункт меню: *Данные — Анализ данных:*

<figure>
    <img src="/media/images/mme_05_02.png" />
</figure>

Укажем инструмент анализа *Регрессия:*

<figure>
    <img src="/media/images/mme_05_03.png" />
</figure>

Появится диалог "Регрессия". Указываем области, где находятся значения $X$, $Y$ (см. рис), указываем, куда выводить результаты и нажимаем кнопку ОК.

<figure>
    <img src="/media/images/mme_05_04.png" />
</figure>

Программа выдаст множество данных по регрессии, в том числе и оценки параметров регрессии:

<figure>
    <img src="/media/images/mme_05_05.png" />
</figure>

В частности, уравнение регрессии:

$$
y = -150,99 + 0,02 x_1 + 1,78 x_2
$$
:::

## Суммы квадратов

Дисперсия зависимой переменной, домноженная на $n$ (или *полная сумма квадратов*), обозначается **TSS:**

$$
TSS = \sum (y_i - \bar{y})^2 = n \mathbb{D}_y^*.
$$

Сумма

$$
RSS = \sum (\hat{y}_i - \bar{y})^2 = n \mathbb{D}_\hat{y}^*
$$

содержит "объяснённую часть дисперсии".

И, наконец, "сумма квадратов остатков"

$$
ESS = \sum (y_i - \hat{y}_i)^2 = \sum e_i^2 = n \mathbb{D}_e^*
$$

представляет собой "необъяснённую регрессией часть дисперсии" зависимой переменной.

Нетрудно проверить, что

$$
RSS + ESS = TSS.
$$

Величина

$$
R^2 = {RSS \over TSS} = 1 - {ESS \over TSS}
$$

называется **коэффициентом детерминации** и представляет собой долю дисперсии зависимой переменной $y$, объяснённую при помощи уравнения регрессии

$$
0 \le R^2 \le 1.
$$

Чем ближе коэффициент детерминации к 1, тем лучше приближаются исходные данные уравнением линейной регресии.

**Замечание.** В модели множественной линейной регрессии, кроме $\beta$, есть ещё один параметр — $\sigma^2$. Его оценку можно найти по формуле

$$
\hat{\sigma}^2 = S^2 = {ESS \over n - m - 1}.
$$

::: info Пример
В рассмотренном примере 1 получили, что
$$
y = -150,99 + 0,02 x_1 + 1,78 x_2.
$$

$$
Y = \begin{pmatrix}
28 \\ 32 \\ 38 \\ 41 \\ 53
\end{pmatrix},
~~~~
\hat{Y} = X \hat{\beta} =
\begin{pmatrix}
1 & 636 & 93 \\
1 & 689 & 95 \\
1 & 753 & 97 \\
1 & 796 & 100 \\
1 & 868 & 104
\end{pmatrix}
\begin{pmatrix}
150,99 \\ 0,02 \\ 1,78
\end{pmatrix}
=
\begin{pmatrix}
27,59 \\ 32,22 \\ 37,09 \\ 43,28 \\ 51,85
\end{pmatrix}.
$$

$$
\bar{Y}
= {28 + 32 + 38 + 41 + 53 \over 5} = 38,4;
$$

$$
RSS = (27,59 - 38,4)^2 + (32,22 - 38,4)^2 + \cdots + (51,85 - 38,4)^2 = 361,609;
$$

$$
TSS = (28 - 38,4)^2 + (32 - 38,4)^2 + \cdots + (53 - 38,4)^2 = 369,24;
$$

$$
ESS = (28 - 27,59)^2 + (32 - 32,22)^2 + \cdots + (53 - 51,85)^2 = 7,5906;
$$

$$
S^2 = {ESS \over 5 - 2 - 1} = 3,7953;
$$

$$
R^2 = {RSS \over TSS} = {361,609 \over 369,24} = 0,9794.
$$
:::

**Замечание.** На MS Excel эти параметры выглядят следующим образом:

<figure>
    <img src="/media/images/mme_05_06.png" />
</figure>

## Основные гипотезы

Гипотезы, лежащие в основе *классической модели множественной регрессии*, следующие:

> **(A)** "Истинная" зависимость $Y$ от регрессоров $X$ имеет вид
> $$
> Y = X \beta + \varepsilon.
> $$

> **(B)** Величины $X$ — детерминированные (не случайные).

> **(C)** Столбцы матрица $X$ линейно зависимы (другими словами, ранг матрицы $X$ равен $m+1$).

> **(D)** $\mathbb{E}\varepsilon_i = 0$, $\mathbb{D} \varepsilon_i = \sigma^2$ не зависит от $i$.

> **(E)** $\text{cov}(\varepsilon_i \varepsilon_j) = 0$ при $i \ne j$ (некоррелированность ошибок для разных наблюдений).

**Замечание.** Вместо (D) часто добавляют условие:

> **(F)** Случайные факторы $\varepsilon_i$ имеют *нормальное* распределение $N(0, \sigma^2)$.

**Замечание.** Условия (D)—(F) удобно записывать в матричной форме:

> **(G)** $\mathbb{E}\varepsilon = 0$, $\mathbb{E}(\varepsilon \varepsilon^T) = \sigma^2 I$, где $I$ — единичная матрица.

> **(F')** $\varepsilon \sim N(0, \sigma^2 I)$.

## Теорема Гаусса-Маркова

:::: tip Теорема
В предположениях (A)—(E) оценки, полученные методом наименьших квадратов по формуле

$$
\hat{\beta} = (X^T X)^{-1} X^T Y,
\tag{∗}
$$

являются *несмещёнными* и обладают *наименьшей дисперсией среди всех линейных несмещённых оценок* параметров $\beta$.

::: info Доказательство

1. <u>МНК — линейная оценка.</u> Это видно непосредственно из формулы $(*)$.

2. <u>МНК — несмещённая оценка:</u>

   $$
   \mathbb{E} \hat{\beta}
   = \mathbb{E} (X^T X)^{-1} X^T Y
   = \mathbb{E}(X^T X)^{-1} X^T (X\beta + \varepsilon) =
   $$

   $$
   = \mathbb{E}(X^T X)^{-1}X^T(X\beta) + \mathbb{E}(X^T X)^{-1} X^T \varepsilon
   = \beta + (X^T X)^{-1} X^T \mathbb{E}\varepsilon
   = \beta.
   $$

3. <u>Дисперсия МНК-оценки:</u>

   **Дисперсия вектора** ($\text{Var}$) — это ковариационная матрица:

   $$
   \hat{\beta} = Ay \implies \text{Var }(\hat{\beta}) = \text{Var }(Ay) = A \text{ Var }(y) A^T,
   $$

   где $A = (X^T X)^{-1} X^T$.

   Далее,

   $$
   \text{Var }(y) = \text{Var }(X\beta + \varepsilon) = \text{Var }(\varepsilon) = \sigma^2 I.
   $$

   Следовательно,

   $$
   \text{Var }(\hat{\beta}) = A(\sigma^2 I) A^T = \sigma^2 AA^T.
   $$

   Подставим $A = (X^T X)^{-1} X^T$:

   $$
   A \cdot A^T = (X^T X)^{-1} X^T \cdot X(X^T X)^{-1} = (X^T X)^{-1}.
   $$

   Итак,

   $$
   \boxed{ ~ \text{Var }(\hat{\beta}) = \sigma^2 (X^T X)^{-1}. ~ }
   $$

4. <u>Рассмотрим любую другую линейную несмещённую оценку:</u>

   Пусть $\tilde{\beta} = Cy$ — другая линейная несмещённая оценка $\beta$, где $C$ — некоторая матрица, отличающаяся от $A$. Так как $\tilde{\beta}$ — несмещённая, то

   $$
   \mathbb{E}\tilde{\beta} = \beta \implies
   \mathbb{E}(Cy) = C \mathbb{E} y = CX \beta.
   $$

   Это должно выполняться для любого $\beta$, значит, $CX = I$.

5. <u>Сравним дисперсии:</u>

   Наша цель — показать, что дисперсия любой другой оценки не меньше, чем у МНК.

   Рассмотрим разницу между оценками:

   $$
   \tilde{\beta} = Cy = C(X\beta + \varepsilon) = CX \beta + C \varepsilon = \beta + C\varepsilon.
   $$

   Аналогично:
   
   $$
   \hat{\beta} = Ay = A(X \beta + \varepsilon) = AX \beta + A \varepsilon = \beta + A \varepsilon.
   $$

   Тогда разность:

   $$
   \tilde{\beta} - \hat{\beta} = (C - A) \varepsilon.
   $$

   Теперь найдём ковариационную матрицу этой разности:

   $$
   \text{Var }(\tilde{\beta} - \hat{\beta})
   = \text{Var }((C - A) \cdot \varepsilon) =
   $$

   $$
   = (C - A) \cdot \text{ Var }(\varepsilon) \cdot (C - A)^T
   = \sigma^2 (C - A) (C - A)^T.
   $$

   Эта матрица — неотрицательно определённая (всегда $\succeq 0$), потому что она имеет вид $\sigma^2 PP^T$.

6. <u>Ключевая идея — разложение дисперсии.</u>

   Используем тождество $\text{Var }(a + b) = \text{Var }(a) + \text{Var }(b) + 2 \text{ cov }(a,b)$:

   $$
   \text{Var }(\tilde{\beta})
   = \text{Var } (\hat{\beta} + (\tilde{\beta} - \hat{\beta}))
   = \text{Var }(\hat{\beta}) + \text{Var }(\tilde{\beta} - \hat{\beta}) + 2 \text{ cov }(\hat{\beta}, \tilde{\beta} - \hat{\beta}).
   $$

   Далее:

   $$
   \text{cov }(\hat{\beta}, \tilde{\beta} - \hat{\beta})
   = \text{cov }(\beta + A \varepsilon, (C - A) \cdot \varepsilon) =
   $$

   $$
   = \text{cov }(A \varepsilon, (C - A) \cdot \varepsilon)
   = \mathbb{E} \left[
    A \varepsilon \varepsilon^T (C - A)^T
   \right] = A \sigma^2 (C - A)^T = (\cdots)
   $$

   Подставим $A = (X^T X)^{-1} X^T$:

   $$
   (\cdots) = \sigma^2 (X^T X)^{-1} X^T (C - A)^T.
   $$

   Вспомним, что $CX = I$ и $A = (X^T X)^{-1} X^T X = I$, значит:

   $$
   (C - A) X = CX - AX = I - I = 0 \implies X^T (C - A)^T = 0.
   $$

   Следовательно:

   $$
   \text{cov }(\hat{\beta}, \tilde{\beta} - \hat{\beta}) = 0
   $$

   и

   $$
   \text{Var }(\tilde{\beta}) = \text{Var }(\hat{\beta}) + \text{Var }(\tilde{\beta} - \hat{\beta}) =
   $$

   $$
   = \text{Var }(\hat{\beta}) + \sigma^2 (C - A)(C - A)^T \ge \text{Var }(\hat{\beta}).
   $$
:::
::::

## Квантили

Формулы, описывающие статистические свойства оценок, как правило содержат *табличные значения*, называемые **квантилями**.

В последних версиях MS Excel квантили можно найти так:

| Квантиль | Функция MS Excel |
|-|-|
| $u_p$ | ``НОРМ.СТ.ОБР(p)`` |
| $\chi_p^2(k)$ | ``ХИ2.ОБР(p, k)`` |
| $t_p(k)$ | ``СТЬЮДЕНТ.ОБР(p, k)`` |
| $F_p(k_1, k_2)$ | ``F.ОБР(p, k₁, k₂)`` |

## Доверительные интервалы

Далее будем считать, что условие нормальности (F) выполнено:

> **(F)** Случайные факторы $\varepsilon_i$ имеют нормальное распределение $N(0, \sigma^2)$.

Тогда можно построить **доверительные интервалы** для параметров регрессии, т. е. интервалы, *содержащие точные значения этих параметров с заданной вероятностью $1 - \alpha$*.

$$
\hat{\beta}_i - t_{1 - {\alpha \over 2}}(n - m - 1) \cdot S_{\beta_i}
< \beta_i
<
\hat{\beta}_i + t_{1 - {\alpha \over 2}}(n - m - 1) \cdot S_{\beta_i};
$$

$$
{ESS \over \chi_{1 - {\alpha \over 2}}^2 (n - m - 1)}
< \sigma^2 <
{ESS \over \chi_{{\alpha \over 2}}^2 (n - m - 1)}.
$$

Здесь $\alpha$ — "уровень значимости", т. е. вероятность того, что построенный интервал *не содержит* истинного значения параметра.

$S_\beta$ — стандартные ошибки параметров:

$$
S_{\beta_i}^2 = S^2 \cdot (X^T \cdot X)^{-1}_{ii}.
$$

::: info Пример
В рассмотренном примере 1:

$$
(X^T X)^{-1} =
\begin{pmatrix}
2452,8 & 1,822 & -39,02 \\
1,822 & 0,0014 & -0,03 \\
-39,02 & -0,03 & 0,625
\end{pmatrix};
$$

$$
y = -150,99 + 0,02 x_1 + 1,78 x_2;
$$

$$
S^2 = 3,7953.
$$

Тогда стандартные ошибки параметров равны

$$
S_{\beta_0}^2 = S^2 \cdot 2452,8 = 9309,16; ~ ~ S_{\beta_0} = 96,48;
$$

$$
S_{\beta_1}^2 = S^2 \cdot 0,0014 = 0,0053; ~ ~ S_{\beta_1} = 0,07;
$$

$$
S_{\beta_2}^2 = S^2 \cdot 0,625 = 2,372; ~ ~ S_{\beta_2} = 1,54.
$$

> **Замечание.** MS Excel вычисляет стандартные ошибки коэффициентов:
>
> <figure>
>    <img src="/media/images/mme_05_05.png" />
> </figure>
>
> Как и сами интервалы для коэффициентов (но не для $\sigma^2$), если указать уровень значимости.

Далее, на уровне значимости $\alpha = 0,1$:

$$
t_{1 - \alpha/2} (n - m - 1) = t_{0,95}(2) = 2,92;
$$

$$
-150,99 - 2,92 \cdot 96,48 = -432,72
< \beta_0 <
130,74 = -150,99 + 2,92 \cdot 96,48.
$$

Аналогично:

$$
-0,19 < \beta_1 < 0,23;
$$

$$
-2,71 < \beta_2 < 6,28.
$$

Наконец,

$$
\chi_{1-\alpha/2}^2(n - m - 1) = \chi_{0,95}^2(2) = 5,99;
$$

$$
\chi_{\alpha/2}^2(n - m - 1) = \chi_{0,05}^2(2) = 0,1;
$$

$$
{ESS \over 5,99} = {7,5906 \over 5,99} \approx 1,27
< \sigma^2 <
75,9 \approx {7,5906 \over 0,1}.
$$

В MS Excel:

<figure>
    <img src="/media/images/mme_05_07.png" />
</figure>
:::

## Интервал для прогнозного значения

Рассмотрим модель $Y = X\beta + \varepsilon$, где

$$
Y = \begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix},
~~~~
X = \begin{pmatrix}
1 & x_{11} & \cdots & x_{m1} \\
1 & x_{12} & \cdots & x_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{1n} & \cdots & x_{mn}
\end{pmatrix},
$$

$$
\beta = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_m
\end{pmatrix},
~~~~
\varepsilon = \begin{pmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_m
\end{pmatrix}.
$$

Напомним, что $X$ — матрица, каждая из $n$ строк которой соответствует какому-то набору значений независимых переменных.

Предположим, что у нас есть ещё один набор значений независимых переменных:

$$
x^{(n+1)} = \begin{pmatrix}
1 & x_1^{(n+1)} & x_2^{(n+1)} & \cdots & x_m^{(n+1)}
\end{pmatrix}.
$$

Требуется найти доверительный интервал для соответствующего значения зависимой переменной $y^{(n+1)}$.

Тогда

$$
\hat{y}^{(n+1)} - t_{1 - {\alpha \over 2}}(n - m - 1) \cdot S_y
< y^{(n+1)} <
\hat{y}^{(n+1)} + t_{1 - {\alpha \over 2}}(n - m - 1) \cdot S_y,
$$

где

$$
\hat{y}^{(n+1)} = \hat{\beta}_0 + \hat{\beta}_1 x_1^{(n+1)} + \cdots + \hat{\beta}_m x_m^{(n+1)} = x^{(n+1)} \cdot \hat{\beta};
$$

$$
S_y = S \cdot \sqrt{
    1 + x^{(n+1)} \cdot (X^T X)^{-1} \cdot (x^{(n+1)})^T
}.
$$

:::: info Пример 2
В таблице приведены значения двух факторов. Построить уравнение линейной регресии и найти доверительный интервал для $Y$ при $X = 7$. Взять $\alpha = 0,1$.

<table>
<tbody>
    <tr>
        <th>X</th>
        <td>1</td>
        <td>2</td>
        <td>3</td>
        <td>6</td>
        <td>8</td>
    </tr>
    <tr>
        <th>Y</th>
        <td>1</td>
        <td>4</td>
        <td>5</td>
        <td>9</td>
        <td>11</td>
    </tr>
</tbody>
</table>

::: info Решение
Запишем данные в матричной форме:
$$
Y = \begin{pmatrix}
1 \\ 4 \\ 5 \\ 9 \\ 11
\end{pmatrix},
~~~~
X = \begin{pmatrix}
1 & 1 \\
1 & 2 \\
1 & 3 \\
1 & 6 \\
1 & 8
\end{pmatrix},
~~~~
\hat{\beta} = \begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}.
$$

Тогда

$$
(X^T X)^{-1} =
\begin{pmatrix}
0,671 & -0,118 \\
-0,118 & 0,029
\end{pmatrix},
$$

$$
y = 0,59 + 1,35x, ~ ~ ~ ~ S^2 = 0,5882
$$

и

$$
\hat{y}^{(n+1)} = \hat{\beta}_0 + \hat{\beta}_1 x_1^{(n+1)} =
\begin{pmatrix}
1 & 7
\end{pmatrix} \cdot
\begin{pmatrix}
0,59 \\ 1,35
\end{pmatrix} = 10,04;
$$

$$
S_y = \sqrt{0,5889} \cdot \sqrt{
    1 + \begin{pmatrix}
1 & 7
\end{pmatrix} \cdot
\begin{pmatrix}
0,671 & -0,118 \\
-0,118 & 0,029
\end{pmatrix} \cdot \begin{pmatrix} 1 \\ 7 \end{pmatrix}
} \approx 0,921;
$$

$$
t_{1 - \alpha/2}(n-m-1) = t_{0,95}(5 - 1 - 1) = 2,35.
$$

Окончательно,

$$
10,04 - 2,35 \cdot 0,921 < y^{(n+1)} < 10,04 + 2,35 \cdot 0,921.
$$
:::
::::

## Значимость уравнения

Мы рассматриваем уравнение регрессии

$$
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_m x_m + \varepsilon.
$$

Говорят, что уравнение значимо, если отвергается гипотеза о *равенстве нулю одновременно всех коэффициентов* при $x$:

$$
H_0: ~ ~ \beta_1 = 0, \beta_2 = 0, \cdots, \beta_m = 0.
$$

Для проверки значимости уравнения используется **F-статистика:**

$$
F = {RSS/m \over ESS/(n - m - 1)} = {R^2 / m \over (1 - R^2)/(n - m - 1)}.
$$

> **Замечание.** В MS Excel F-статистика вычисляется вместе с регрессией.
> 
> <figure>
>     <img src="/media/images/mme_05_08.png" />
> </figure>

Если $F \ge F_{1 - \alpha}(m, n - m - 1)$, то уравнение регрессии *значимо* при выбранном $\alpha$.

:::: info Задача
По выборке


| $y$ | $x_1$ | $x_2$ |
|-|-|-|
| 6 | 2 | 3 |
| 10 | 2 | 4 |
| 10 | 5 | 7 |
| 11 | 7 | 10 |
| 3 | 7 | 6 |

получено уравнение линейной регресии:

$$
y = 4,8 - 2,15 x_1 + 2,18 x_2.
$$

Значимо ли это уравнение на уровне 0,1?

::: info Решение
1. Вычислим расчётные значения $Y$:
   
   | $\hat{y}$ |
   |-|
   | 7,04 |
   | 9,22 |
   | 9,31 |
   | 11,55 |
   | 2,83 |

2. Вычислим RSS и ESS:

   $$
   RSS = 43,5;
   $$

   $$
   ESS = 2,5.
   $$

3. Вычислим $F$:

   $$
   F = {RSS / m \over ESS / (n - m - 1)}
   = {43,5 / 2 \over 2,5 ( 5 - 2 - 1)}
   = 17,4.
   $$

Квантиль $F_{0,9}(2; 5 - 2 - 1) = F_{0,9}(2;2)$ по таблице равен $9$.

так как $17,4 > 9$, то уравнение значимо.
:::
::::

> **Замечание.** Значимость уравнения удобнее всего оцениать, используя величину *значимость F*.
> 
> Это — минимальное значение уровня значимости, на котором уравнение ещё занчимо.
> 
> <figure>
>     <img src="/media/images/mme_05_09.png" />
> </figure>

Уравнение *значимо*, если значимость $F \le \alpha$.