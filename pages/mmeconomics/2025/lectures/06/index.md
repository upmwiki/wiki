---
prev:
   text: 5. Классическая модель множественной линейной регрессии
   link: ../05/
next:
   text: 7. Нарушения допущений классической линейной модели
   link:  ../07/
---

**Математические модели в экономике**

# 6. Нелинейные модели

<p class="subtext">Лекция</p>

## Нелинейные формы зависимости

Использование лишь линейных зависимостей для описания экономических взаимосвязей часто оказывается недостаточным. Необходимо использовать и нелинейные соотношения.

Например, *производственная функция Кобба-Дугласа:*

$$
Y = A \cdot K^\alpha \cdot L^\beta,
$$

где $Y$ — выпуск, $K$ — затраты капитала, $L$ — затраты труда; $A$, $\alpha$, $\beta$ — параметры.

:::: info Пример
По исходным данным оценить коэффициенты $A$, $\alpha$, $\beta$ в формуле Кобба-Дугласа

$$
Y = A \cdot K^\alpha \cdot L^\beta \cdot (1 + \varepsilon).
$$

| $Y$ | $K$ | $L$ |
|-|-|-|
| 34 | 2 | 4 |
| 53 | 3 | 6 |
| 81 | 5 | 7 |
| 81 | 5 | 7 |
| 113 | 7 | 9 |
| 97 | 9 | 4 |

::: info Решение
Логарифмированием зависимость приводится к линейному виду:

$$
\ln Y = \ln A + \alpha \cdot \ln K + \beta \cdot \ln L + \ln (1 + \varepsilon).
$$

| $\ln Y$ | $\ln K$ | $\ln L$ |
|-|-|-|
| 3,53 | 0,69 | 1,39 |
| 3,96 | 1,1 | 1,79 |
| 4,39 | 1,61 | 1,95 |
| 4,39 | 1,61 | 1,95 |
| 4,73 | 1,95 | 2,2 |
| 4,57 | 2,2 | 1,39 |

Далее как обычно:

$$
\begin{pmatrix}
\ln A \\ \alpha \\ \beta
\end{pmatrix} =
\begin{pmatrix}
2,48 \\ 0,69 \\ 0,41
\end{pmatrix}.
$$

$$
\ln Y = 2,48 + 0,69 \cdot \ln K + 0,41 \cdot \ln K.
$$

$$
A = e^{2,48} \approx 12.
$$

**Ответ:** $Y = 12 \cdot K^{0,69} \cdot L^{0,41} \cdot (1 + \varepsilon)$.
:::
::::

:::: info Пример (полиномиальная зависимость)
По исходным данным оценить коэффициенты в формуле

$$
y = a + bx + cx^2.
$$

| $y$ | $x$ |
|-|-|
| 5 | 2 |
| 3 | 3 |
| 3 | 5 |
| 2 | 5 |
| 2 | 7 |
| 1 | 8 |

::: info Решение
Заменой $z = x^2$ зависимость приводится к линейному виду $y = a + bx + cz$.

| $y$ | $x$ | $z = x^2$ |
|-|-|-|
| 5 | 2 | 4 |
| 3 | 3 | 9 |
| 3 | 5 | 25 |
| 2 | 5 | 25 |
| 2 | 7 | 49 |
| 1 | 8 | 64 |

Далее как обычно:

$$
\begin{pmatrix}
a \\ b \\ c
\end{pmatrix} =
\begin{pmatrix}
6,54 \\ -1,11 \\ 0,06
\end{pmatrix}.
$$

$$
Y = 6,54 - 1,11 X + 0,06 Z.
$$

**Ответ:** $y = 6,54 - 1,11 x + 0,06 x^2$.
:::
::::

::: info Пример
В таблице перечислены некоторые модели мотоциклов, их пробег (в километрах на литр) и вес (в килограммах).

| Motorcycle | Mileage | Dry Weight |
|-|-|-|
| RSV Mille | 16,77 | 220,5 |
| RSV Tuono | 17,2 | 214,2 |
| Mana 850 | 21,07 | 232,2 |
| R1150RS | 17,2 | 261,9 |
| K1200GT | 16,34 | 299,7 |
| R1200GS | 19,78 | 261,9 |
| CBR600RR | 15,48 | 193,05 |
| ST1300 | 16,77 | 328,95 |
| CBR300R | 23,64 | 161,55 |
| Ninja 300 | 25,8 | 173,7 |

<figure>
    <img src="/media/images/mme_05_10.png" />
</figure>

Добавим линию тренда.

Линия тренда по умолчанию линейная, однако в диалоговом окне "Линия тренда" есть возможность добавить другие тренды. Мы выберем "Полиномиальную". Как только мы выберем "Полиномиальный", рядом с ним появится поле "Порядок", и мы сможем установить порядок нужного уравнения.

<figure>
    <img src="/media/images/mme_06_01.png" />
</figure>
<br />
<figure>
    <img src="/media/images/mme_06_02.png" />
</figure>

Здесь выбрано уравнение регрессии третьей степени.
:::

:::: info Пример (гиперболическая зависимость)
По исходным данным оценить коэффициенты в формуле

$$
y = a + {b \over x} + \varepsilon.
$$

| $y$ | $x$ |
|-|-|
| 5 | 2 |
| 3 | 3 |
| 3 | 5 |
| 2 | 5 |
| 2 | 7 |
| 1 | 8 |

::: info Решение
Заменой $z = {1 \over x}$ зависимость приводится к линейному виду $y = a + bz + \varepsilon$.

| $y$ | $z = {1 \over x}$ |
|-|-|
| 5 | 0,5 |
| 3 | 0,3333 |
| 3 | 0,2 |
| 2 | 0,2 |
| 2 | 0,14286 |
| 1 | 0,125 |

Далее как обычно:

$$
y = 0,43 + 8,94 z + \varepsilon.
$$

**Ответ:** $y = 0,43 + {8,94 \over x} + \varepsilon$.
:::
::::

## Нелинейные модели, не приводимые к линейному виду

Всё же далеко не всякую зависимость заменой переменных можно привести к линейному виду.

В общем случае уравнение нелинейной регрессии с аддитивным случайным членом $\varepsilon$ имеет вид

$$
y = f(x_1, ..., x_m, \beta_0, ..., \beta_k) + \varepsilon,
$$

где $\beta_0, ..., \beta_k$ — параметры модели.

Для нахождения оценок параметров можно использовать, как и в линейном случае, **метод наименьших квадратов**, но не матричную формулу $(∗)$.

$$
ESS = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \to \min
$$

:::: info Пример
По исходным данным оценить коэффициенты $a$, $b$ в формуле

$$
y = a \cdot e^{bx} + \varepsilon.
$$

| $y$ | $x$ |
|-|-|
| 2 | 1 |
| 9 | 2 |
| 47 | 3 |

::: info Решение
Задача не сводится к линейной.

Выпишем формулу для ESS:

$$
ESS = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = (2 - ae^b)^2 + (9 - ae^{2b})^2 + (47 - ae^{3b})^2.
$$

Нужно найти *минимум* этой функции. Используем надстройку *Поиск решения* из MS Excel. Внесём все данные и формулы в таблицу:

<figure>
    <img src="/media/images/mme_06_03.png" />
</figure>

Начальные значения $a$ и $b$ положены равными нулю.

Вызовем надстройку *Поиск решения* и введём данные в диалоге.

<figure>
    <img src="/media/images/mme_06_04.png" />
</figure>

Таким образом, оценки параметров модели: $a = 0,34$, $b = 1,642$.

$$
y = 0,34 \cdot e^{1,642 x} + \varepsilon.
$$
:::
::::

Важно понимать, что не существует правил относительно линейных и нелинейных моделей.

В целом, лучше всегда начинать с линейной модели, а затем проверить её пригодность. После этого вы можете построить нелинейную модель на тех же данных и проверить её соответствие, чтобы увидеть, улучшилось ли оно. Если нелинейная модель значительно улучшает подгонку по сравнению с линейной моделью, то лучше использовать нелинейную модель.

Выбор между линейной и нелинейной моделями для данного набора данных — это компромисс между соответствием модели (что влияет на точность прогноза, сделанного на основе модели) и сложностью построения и анализа.

## Спецификация модели

Спецификация модели включает следующие этапы:

1. отбор факторов;
2. выбор функциональной формы модели.

### Отбор факторов

Нередко имеется большое число факторов, но неясно, какие именно переменные включать в модель. В этом случае используются различные **эвристические процедуры**.

Далее приведён простой пример такой процедуры.

::: tip Замечание
При добавлении в модель новых объясняющих переменных коэффициент $R^2$ *не может уменьшиться*. Поэтому *при добавлении новой переменной* неясно, за счёт чего возрос $R^2$: за счёт реального влияния дополнительной переменной или просто из-за возрастания числа переменных.

Для того чтобы можно было как-то *сравнивать модели с разным числом переменных*, вводят так называемый **скорректированный** (или **нормированный**) **коэффициент детерминации:**

$$
R^2_\text{норм} = 1 - { ESS / (n - m - 1) \over TSS / (n - 1) }
$$

Это число также не превосходит 1, но в некоторых случаях может оказаться и отрицательным.
:::

**Процедура пошагового отбора независимых переменных:**

1. Из исходного набора переменных включается в модель переменная, имеющая *наибольший по модулю коэффициент корреляции с зависимой переменной $Y$*.
2. На каждом последующем шаге в модель добавляется та из переменных, добавление которой *максимально увеличивает скорректированный коэффициент детерминации*.
3. Если добавление новых переменных не увеличивает этот коэффициент, процедура считается завершённой.

:::: info Пример
Произвести пошаговый отбор переменных регрессии для выборки.

| $Y$ | $X_1$ | $X_2$ | $X_3$ | $X_4$ |
|-|-|-|-|-|
| 15 | 1 | 17 | 41 | 31 |
| 27 | 4 | 11 | 41 | 10 |
| 32 | 7 | 4 | 22 | 8 |
| 31 | 8 | 8 | 37 | 9 |
| 36 | 10 | 8 | 46 | 17 |
| 50 | 12 | 7 | 43 | 8 |
| 23 | 11 | 11 | 41 | 10 |
| 16 | 5 | 14 | 34 | 5 |
| 52 | 19 | 7 | 50 | 7 |
| 19 | 23 | 5 | 15 | 6 |

::: info Решение
1. Найдём матрицу корреляций:
   
   <figure>
       <img src="/media/images/mme_06_05.png" />
   </figure>

   Наибольший *по модулю* коэффициент корреляции с $Y$ имеет переменная $X_2$.

2. Строим регрессию $Y$ на $X_2$:

   $$
   y = 46,75 - 1,81 x_2, ~~ R^2_\text{корр} = 0,232.
   $$

3. Будем добавлять к $X_2$ каждую из переменных $X_1$, $X_3$, $X_4$:

   1. Добавим к $X_2$ переменную $X_1$:

      $$
      (X_2; X_1): ~~~ y = 46,21 + 0,027 x_1 - 1,78 x_2, ~~ R^2_\text{корр} = 0,122.
      $$
   2. Теперь вариант $(X_2; $X_3)$:
      $$
      y = \cdots, ~~ R^2_\text{корр} = 0,852.
      $$
   3. Наконец, вариант $(X_2; X_4)$:
      $$
      y = \cdots, ~~ R^2_\text{корр} = 0,126.
      $$

   $R^2_\text{корр}$ сильнее возрос в варианте $(X_2; X_3)$. Оставляем эти переменные.

4. Добавим к $(X_2; X_3)$ другие переменные:

   1. Добавим к этому варианту $(X_2; X_3)$ переменную $X_1$:

      $$
      (X_2; X_3; X_1): ~~~ y = \cdots, ~~ R^2_\text{корр} = 0,828.
      $$

      $R^2_\text{корр}$ уменьшился по сравнению с предыдущим вариантом.

   2. Добавим к варианту $(X_2; X_3)$ переменную $X_4$:

      $$
      (X_2; X_3; X_4): ~~~ y = \cdots, ~~ R^2_\text{корр} = 0,829.
      $$

   Все варианты с тремя переменными оказались *хуже*, чем вариант с двумя переменными $(X_2; X_3)$. Отбор переменных закончен.

**Ответ:** результат пошагового отбора переменных

$$
y = 19,59 - 2,8 x_2 + 0,98 x_3, ~~~ R^2_\text{корр} = 0,852.
$$
:::
::::

## Проверка гипотезы о линейной связи коэффициентов

Пусть рассматривается линейная модель

$$
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_m x_m + \varepsilon.
\tag{U}
$$

Мы хотим провериь гипотезу:

> $H_0$: коэффициенты модели удовлетворяют $q$ линейным ограничениям.

Вот, например, три ограничения ($q = 3$):

$$
H_0: ~~ \begin{cases}
\beta_1 = 2 \beta_2, \\
\beta_3 = 0, \\
\beta_4 = 5.
\end{cases}
$$

Чтобы проверить такую гипотезу, нужно:

1. Вычислить $ESS_U$ для исходной регрессии.
2. Вычислить $ESS_R$ для регрессии, в которой выполнены ограничения.
3. Вычислить

   $$
   F = { (ESS_R - ESS_U) / q \over ESS_U / (n - m - 1) }
   $$

Гипотеза $H_0$ об ограничениях *принимается*, если

$$
F < F_{1 - \alpha}(q, n - m - 1).
$$

::: info Пример на Python
Набор данных о ценах на жильё в Калифорнии. Построим линейную регрессионную модель, которая предсказывает цену на жильё в зависимости от нескольких факторов.

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.datasets import fetch_california_housing

# Загрузка данных о жилье в Калифорнии
california = fetch_california_housing()

# Преобразование в DataFrame
df = pd.DataFrame(california.data, columns=california.feature_names)

# Добавление зависимой переменной (цены на жильё)
df['MedHouseVal'] = california.target

# Просмотр первых строк данных
df.head()
```

Первые 20 строк данных:

```:no-line-numbers
MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
0   8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
1   8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
2   7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
3   5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
4   3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   
5   4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   
6   3.6591      52.0  4.931907   0.951362      1094.0  2.128405     37.84   
7   3.1200      52.0  4.797527   1.061824      1157.0  1.788253     37.84   
8   2.0804      42.0  4.294118   1.117647      1206.0  2.026891     37.84   
9   3.6912      52.0  4.970588   0.990196      1551.0  2.172269     37.84   
10  3.2031      52.0  5.477612   1.079602       910.0  2.263682     37.85   
11  3.2705      52.0  4.772480   1.024523      1504.0  2.049046     37.85   
12  3.0750      52.0  5.322650   1.012821      1098.0  2.346154     37.85   
13  2.6736      52.0  4.000000   1.097701       345.0  1.982759     37.84   
14  1.9167      52.0  4.262903   1.009677      1212.0  1.954839     37.85   
15  2.1250      50.0  4.242424   1.071970       697.0  2.640152     37.85   
16  2.7750      52.0  5.939577   1.048338       793.0  2.395770     37.85   
17  2.1202      52.0  4.052805   0.966997       648.0  2.138614     37.85   
18  1.9911      50.0  5.343675   1.085919       990.0  2.362768     37.84   
19  2.6033      52.0  5.465455   1.083636       690.0  2.509091     37.84   

    Longitude  MedHouseVal  
0     -122.23        4.526  
1     -122.22        3.585  
2     -122.24        3.521  
3     -122.25        3.413  
4     -122.25        3.422  
5     -122.25        2.697  
6     -122.25        2.992  
7     -122.25        2.414  
8     -122.26        2.267  
9     -122.25        2.611  
10    -122.26        2.815  
11    -122.26        2.418  
12    -122.26        2.135  
13    -122.26        1.913  
14    -122.26        1.592  
15    -122.26        1.400  
16    -122.27        1.525  
17    -122.27        1.555  
18    -122.26        1.587  
19    -122.27        1.629
```

Описание набора данных:

```python
print(california.DESCR)
```

Вывод:

```:no-line-numbers
.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

:Number of Instances: 20640

:Number of Attributes: 8 numeric, predictive attributes and the target

:Attribute Information:
    - MedInc        median income in block group
    - HouseAge      median house age in block group
    - AveRooms      average number of rooms per household
    - AveBedrms     average number of bedrooms per household
    - Population    block group population
    - AveOccup      average number of household members
    - Latitude      block group latitude
    - Longitude     block group longitude

:Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. rubric:: References

- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
  Statistics and Probability Letters, 33 (1997) 291-297
```

Построим линейную регрессионную модель, предсказывающую медианную стоимость жилья на основе таких факторов, как средний доход, средний возраст жилья и другие.

```python
# Выбор независимых переменных для модели
X = df[['AveRooms', 'AveOccup', 'MedInc', 'HouseAge']]
X = sm.add_constant(X)

# Зависимая переменная (цена на жильё)
y = df['MedHouseVal']

# Построение модели
model = sm.OLS(y, X).fit()

# Вывод результатов модели
print(model.summary())
```

Вывод:

```:no-line-numbers
OLS Regression Results                            
==============================================================================
Dep. Variable:            MedHouseVal   R-squared:                       0.514
Model:                            OLS   Adj. R-squared:                  0.514
Method:                 Least Squares   F-statistic:                     5450.
Date:                Fri, 10 Oct 2025   Prob (F-statistic):               0.00
Time:                        22:18:03   Log-Likelihood:                -24802.
No. Observations:               20640   AIC:                         4.961e+04
Df Residuals:                   20635   BIC:                         4.965e+04
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0314      0.022      1.428      0.153      -0.012       0.074
AveRooms      -0.0273      0.002    -11.330      0.000      -0.032      -0.023
AveOccup      -0.0045      0.001     -8.267      0.000      -0.006      -0.003
MedInc         0.4433      0.003    141.672      0.000       0.437       0.449
HouseAge       0.0169      0.000     37.359      0.000       0.016       0.018
==============================================================================
Omnibus:                     4419.990   Durbin-Watson:                   0.807
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            11655.492
Skew:                           1.158   Prob(JB):                         0.00
Kurtosis:                       5.862   Cond. No.                         126.
==============================================================================
```

Теперь проверим гипотезу

$$
H_0: ~~ \beta_\text{AveRooms} = \beta_\text{AveOccup}.
$$

```python
# Проверка гипотезы H0: beta_AveRooms = beta_AveOccup
hypothesis = 'AveRooms = AveOccup'
f_test = model.f_test(hypothesis)

# Вывод статистики F-теста
print(f"F-статистика: {f_test.fvalue}")
print(f"p-value: {f_test.pvalue}")
```

Вывод:

```:no-line-numbers
F-статистика: 85.99053421018671
p-value: 1.9810384346509656e-20
```
:::

## RESET-тест Рамсея (Ramsey)

**RESET-тест Рамсея** (сокр. от англ. *Regression Equation Specification Error Test*) отвечает на вопрос, надо ли включать в регрессию дополнительные (в частности, нелинейные) члены.

Пусть рассматривается линейная модель:

$$
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_m x_m + \varepsilon.
\tag{∗}
$$

Мы хотим выяснить, не следует ли вместо линейной зависимости $(∗)$ рассмотреть более сложную модель, *включающую степени $X$.

### Алгоритм теста Рамсея

1. Найти оценки параметров регрессии $(∗)$.
2. Вычислить расчётные значения $\hat{y}$ зависимой переменной $y$.
3. Оценить регрессию:

   $$
   y = \beta_0 + \beta_1 x_1 + \cdots + \beta_m x_m + \alpha_2 \hat{y}^2 + \cdots + \alpha_k \hat{y}^k + \varepsilon.
   \tag{∗∗}
   $$
4. Проверить гипотезу $H_0:$ все $\alpha_j = 0$ (то есть что дополнительные слагаемые в $(∗∗)$ н нужны, зависимость можно считать линейной).

   Для этого вычислить

   $$
   F = {ESS_* - ESS_{**} / (k - 1) \over ESS_{∗∗} / (n - m - k)}.
   $$

Гипотеза о линейности *принимается*, если

$$
F < F_{1-\alpha} (k - 1, n - m - k).
$$

:::: info Пример
Заработная плата в Нидерландах. Имеется 150 наблюдений, 75 мужчин и 75 женщин, работавших на полную ставку.
* $W$ — заработная плата (гульденов в час),
* $SEX$ — пол (1 — мужчины, 2 — женщины),
* $EDU$ — уровень образования (1 — нач. школа, ..., 5 — университет);
* $AGE$ — возраст.

| | W | SEX | EDU | AGE |
|-|-|-|-|-|
| 1 | 10,44 | 1 | 1 | 19 |
| 2 | 13,52 | 1 | 1 | 20 |
| 3 | 19,12 | 1 | 1 | 21 |
| 4 | 20,28 | 1 | 1 | 25 |
| 5 | 14,63 | 1 | 1 | 26 |
| ... | ... | ... | ... | ... |
| 22 | 20,32 | 1 | 2 | 29 |
| 23 | 15,96 | 1 | 2 | 32 |
| 24 | 17,07 | 1 | 2 | 37 |
| 25 | 13,04 | 1 | 2 | 39 |
| ... | ... | ... | ... | ... |
| 148 | 26,09 | 2 | 5 | 27 |
| 149 | 24,95 | 2 | 5 | 37 |
| 150 | 41,05 | 2 | 5 | 52 |

Оценим линейную регрессию:

$$
W = 3,51 - 3,55 \cdot SEX + 3,24 \cdot EDU + 0,44 \cdot AGE,
$$

$$
ESS = 7431.
$$

Проверим при помощи теста Рамсея правильность функциональной нормы. Для этого вычислим столбец $\hat{W}$ расчётных значений и добавим к исходным регрессорам его вторую степень (ограничимся второй степенью).

| | W | SEX | EDU | AGE | W^2 | | W^ |
|-|-|-|-|-|-|-|-|
| 1 | 10,44 | 1 | 1 | 19 | 134,486 | | 11,597 |
| 2 | 13,52 | 1 | 1 | 20 | 144,921 | | 12,038 |
| 3 | 19,12 | 1 | 1 | 21 | 155,745 | | 12,480 |
| 4 | 20,28 | 1 | 1 | 25 | 202,939 | | 14,246 |
| 5 | 14,63 | 1 | 1 | 215,712 | | 14,687 |
| ... | ... | ... | ... | ... | | ... |

Получим уравнение регрессии:

$$
W = 12,09 + 0,29 \cdot SEX - 0,49 \cdot EDU - 0,09 \cdot AGE + 0,02 \cdot \hat{W}^2,
$$

$$
ESS = 7154.
$$

Вычислим F-статистику:

$$
F = {(ESS_* - ESS_{**}) / (k - 1) \over ESS_{**} / (n - m - k)}
= {(7631 - 7154)/(2 - 1) \over 7154/(150 - 3 - 2)} = 9,67.
$$

Далее, на уровне значимости $\alpha = 0,1$:

$$
F_{1 - \alpha}(k - 1; n - m - k) = F_{0,9}(1; 145) = 2,75 < 9,67.
$$

Мы видим, что гипотеза о правильности данной формы регрессии отвергается.

В уравнение

$$
W = \beta_0 + \beta_1 \cdot SEX + \beta_2 \cdot EDU + \beta_3 \cdot AGE + \varepsilon
$$

надо попробовать включить нелинейные члены, например, $AGE^2$ или другие переменные.
::::

## Фиктивные переменные

Как правило, независимые переменные принимают непрерывный ряд значений (цена товара, уровень инфляции и т. д.)

Однако нередко приходится учитывать *качественные* признаки.

например, можно изучать влияние пола работника на уровень его зарплаты (переменная "пол" принимает всего два значения), или спросом на прохладительные напитки в зависимости от времени года *(сколько различных значений принимает эта переменная?)*

В подобных ситуациях "качественному" значению условно присваивается числовая метка, как правило, это 0 или 1.

Если качественный признак принимает два значения, то это присвоение можно сделать произвольно (например, "наличие детей" = 1, "отсутствие детей" = 0). Если же качественный признак принимает $k$ значений ($k > 2$), то вводят $k - 1$ переменную, каждая из которых принимает значения 0 и 1.

::: info Пример
Качественный признак — *время года*. Значения: зима, весна, лето, осень.

Фиктивные переменные:

* $X_1 = 1$, если зима, иначе $X_1 = 0$;
* $X_1 = 1$, если весна, иначе $X_1 = 0$;
* $X_1 = 1$, если лето, иначе $X_1 = 0$.

Переменную $X_4$ аналогичным образом вводить бессмысленно (и нельзя!), так как всегда

$$
X_1 = 1 - X_1 - X_2 - X_3.
$$
:::

::: info Пример
Предположим, у нас есть следующий набор данных, и мы хотели бы использовать *семейное положение* и *возраст* для прогнозирования *дохода*.

| Income | Age | Marital Status |
|-|-|-|
| $45,000 | 23 | Single |
| $48,000 | 25 | Single |
| $54,000 | 24 | Single |
| $57,000 | 29 | Single |
| $65,000 | 38 | Married |
| $69,000 | 36 | Single |
| $78,000 | 40 | Married |
| $83,000 | 59 | Divorced |
| $78,000 | 40 | Married |
| $83,000 | 59 | Divorced |
| $98,000 | 56 | Divorced |
| $104,000 | 64 | Married |
| $107,000 | 53 | Married |

...<!-- тут ещё должен быть абзац, он был примерно на две-три строки -->

Поскольку *семейное положение* — это категориальная переменная, которая может принимать три разных значения ("холост", "женат" или "разведён"), нам нужно создать $k - 1 = 3 - 1 = 2$ фиктивные переменные.

Мы можем сделать "Single" быть базовым значением, поскольку оно встречается чаще всего. Таким образом, вот как можно преобразовать *семейное положение* в фиктивные переменные:

| Income | Age | Married | Divorced |
|-|-|-|-|
| $45,000 | 23 | 0 | 0 |
| $48,000 | 25 | 0 | 0 |
| $54,000 | 24 | 0 | 0 |
| $57,000 | 29 | 0 | 0 |
| $65,000 | 38 | 1 | 0 |
| $69,000 | 36 | 0 | 0 |
| $78,000 | 40 | 1 | 0 |
| $83,000 | 59 | 0 | 1 |
| $98,000 | 56 | 0 | 1 |
| $104,000 | 64 | 1 | 0 |
| $107,000 | 53 | 1 | 0 |

Пусть получилась следующая регрессия:

<figure>
    <img src="/media/images/mme_06_06.png" />
</figure>

$$
~ \text{Доход} = 14~276,21 + 1~471,67 \cdot (\text{возраст}) + ~
$$
$$
~ + 2~479,75 \cdot (\text{замужем}) - 8~397,40 \cdot (\text{разведён}). ~
$$

Вот как интерпретировать коэффициенты регрессии:

* *Возраст:* каждый год увеличения возраста связан со средним увеличением дохода на 1471,67 доллара. Поскольку p-значение (0,00) меньше 0,05, возраст является статистически значимым предиктором дохода.
* *Женат:* женатый человек в среднем зарабатывает на 2479,75 долларов больше, чем одинокий человек. Поскольку p-значение (0,80) не менее 0,05, эта разница не является статистически значимой.
* *Разведён:* разведённый человек в среднем зарабатывает на 8397,40 долларов меньше, чем одинокий человек. Поскольку p-значение (0,53) не менее 0,05, эта разница не является статистически значимой.
:::

## Тест Чоу (Chow)

Нередко в некоторый момент можно наблюдать изменение вида фнукциональной зависимости. В таком случае уравнения на разных участках будут различаться.

<figure>
    <img src="/media/images/mme_06_07.png" />
</figure>

В ситуации на рисунке возникает *альтернатива* — описывать регрессию одной синей линией или ломаной из двух линий: красной и зелёной.

Другими словами, пусть выборка состоит из нескольких частей. Требуется выяснить, *следует ли их объединить в одну,* или *рассматривать каждую подвыборку отдельно*.

Обозначим регрессию по первой выборке (объёма $n_1$) через $A$, по второй (объёма $n_2$) — через $B$, а объединённую (объёма $n = n_1 + n_2$) — через $R$. Ясно, что

$$
ESS_A + ESS_B \le ESS_R.
$$

Если левая часть неравенства близка к правой, то это говорит о том, что регрессия по объединённой выборке почти так же хороша, как "составная" их двух регрессий по двум выборкам. В этом случае объединение выборок в одну допустимо.

::: info Пример
Пусть некоторый исследователь изучает взаимосвязь между доходом ($Y$) и уровнем образования ($X$) в течение двух отдельных периодов: до и после значительной реформы политики в области образования. Он делит набор данных на две группы — дореформенную и послереформенную.

*Нулевая гипотеза* заключается в то, что нет структурного разрыва в связи между доходом и образованием, что подразумевает, что коэффициенты одинаковы в обоих периодах.

*Альтернативная гипотеза* предполагает структурный разрыв. Если можно отвергнуть нулевую гипотезу, это будет означать, что реформа политики в области образования существенно повлияла на связь между доходом и образованием.
:::

### Алгоритм теста Чоу

Вычисляется

$$
F = {
   (ESS_R - ESS_A - ESS_B) / (m + 1)
   \over
   (ESS_A + ESS_B) / (n - 2m - 2)
}.
$$

*Возможность объединения* выборок в одну принимается, если

$$
F < F_{1 - \alpha} (m + 1, n - 2m - 2).
$$

:::: info Пример (способ 1)
Проверить возможность объединения выборок при $\alpha = 0,1$.

| $y$ | $x_1$ | $x_2$ |
|-|-|-|
| 3 | 2 | 3 |
| 5 | 3 | 4 |
| 5 | 3 | 6 |
| 6 | 4 | 1 |
| 7 | 5 | 7 |

| $y$ | $x_1$ | $x_2$ |
|-|-|-|
| 9 | 4 | 7 |
| 9 | 6 | 8 |
| 10 | 6 | 9 |
| 12 | 7 | 9 |

::: info Решение
Первый способ (критерий Чоу).

Для первой выборки:

$$
y = 0,86 + 1,26 x_1 + 0,01x_2, ~~~ ESS = 0,42.
$$

Для второй выборки:

$$
y = 3 + 0,5 x_1 + 0,5 x_2, ~~~ ESS = 2,5.
$$

Для объединённой выборки:

$$
y = 0,42 + 1,38 x_1 + 0,2 x_2, ~~~ ESS = 7,66.
$$

Вычислим F:

$$
F = {
   (7,66 - 0,42 - 2,5) / (2 + 1)
   \over
   (0,42 + 2,5) / (9 - 2 \cdot 2 - 2)
} = 1,57.
$$

$$
F_{0,9} (2 + 1; 9 - 2 \cdot 2 - 2) = F_{0,9}(3;3) = 5,4.
$$

Поскольку $1,57 < 5,4$, то объединение выборок возможно.
:::
::::

**Замечание.** Другой хороший способ проверки состоит в использовании фиктивной переменной.

Добавим в модель фиктивную переменную $z$, равную 0 для первой выборки и 1 для второй, а также переменные $zx_1$, $zx_2$, ... . Если все переменные окажутся *незначимыми*, то объединение выборок вохможно.

:::: info Пример (способ 2)
Добавляем новые переменные $z$, $zx_1$, $zx_2$:

| $y$ | $x_1$ | $x_2$ | $z$ | $zx_1$ | $zx_2$ |
|-|-|-|-|-|-|
| 3 | 2 | 3 | 0 | 0 | 0 |
| 5 | 3 | 4 | 0 | 0 | 0 |
| 5 | 3 | 6 | 0 | 0 | 0 |
| 6 | 4 | 1 | 0 | 0 | 0 |
| 7 | 5 | 7 | 0 | 0 | 0 |
| 9 | 4 | 7 | 1 | 4 | 7 |
| 9 | 6 | 8 | 1 | 6 | 8 |
| 10 | 6 | 9 | 1 | 6 | 9 |
| 12 | 7 | 9 | 1 | 7 | 9 |

Проверяем их значимость:

$$
\begin{matrix}
y & = & 0,86 & + & 1,26 x_1 & + & 0,01 x_2 & + & 2,14 z & + & 0,76 zx_1 & + & 0,49 zx_2 \\
\small (p-\text{знач.}) && \small (0,6) && \small (0,07) && \small (0,96) && \small \textbf{(0,76)} && \small \textbf{(0,54)} && \small \textbf{(0,75)}
\end{matrix}
$$

Все добавленные переменные *незначимы* уже на уровне 0,1, так что объединение возможно.
::::