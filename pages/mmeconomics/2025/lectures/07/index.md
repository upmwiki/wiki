---
prev:
    text: 6. Нелинейные модели
    link: ../06/
next: false
---

**Математические модели в экономике**

# 7. Нарушения допущений классической линейной модели

<p class="subtext">Лекция</p>

## Напоминание

Гипотезы, лежащие в основе *классической модели множественной регрессии*, следующие:

> **(A)** "Истинная" зависимость $Y$ от регрессоров $X$ имеет вид
> $$
> Y = X \beta + \varepsilon.
> $$

> **(B)** Величины $X$ — детерминированные (не случайные).

> **(C)** Столбцы матрица $X$ линейно зависимы (другими словами, ранг матрицы $X$ равен $m+1$).

> **(D)** $\mathbb{E}\varepsilon_i = 0$, $\mathbb{D} \varepsilon_i = \sigma^2$ не зависит от $i$.

> **(E)** $\text{cov}(\varepsilon_i \varepsilon_j) = 0$ при $i \ne j$ (некоррелированность ошибок для разных наблюдений).

**Замечание.** Вместо (D) часто добавляют условие:

> **(F)** Случайные факторы $\varepsilon_i$ имеют *нормальное* распределение $N(0, \sigma^2)$.

## Мультиколлинеарность

Одним из важнейший условий является *предположение о линейной независимости столбцов матрицы $X$*.

Нарушение этого условия — *линейная зависимость столбцов $X$* — называется (точной) мультиколлинеарностью.

На практике более реальна ситуация, когда между объясняющими переменными существует не точная линейная, а *сильная корреляционная зависимость*. Эту ситуацию мы и будем называть *мультиколлинеарностью*.

::: info Пример
Пусть $Y$ — величина заработной платы, а среди регрессоров присутствуют:
* $X_1$ — возраст,
* $X_2$ — стаж работы,
* $X_3$ — время обучения,

и другие.

Тогда, очевидно, сумма *$X_2$ и $X_3$* имеет сильную корреляционную связь с *$X_1$ плюс константа* — типичная мультиколлинеарность.
:::

Коэффициент при любом регрессоре (т. е. переменной $X$) представляет "чистое" влияние его на зависимую переменную $Y$.

"Чистое" — означает "при неизменности других регрессоров модели".

Но при мультиколлинеарности два или более регрессоров действуют "в унисон". При изменении одного другой коррелированный с ним меняется тоже.

Поэтому становится затруднительно выделить "чистое" воздействие на $Y$ каждого по отдельности. Это приводит к проблемам.

### Последствия мультиколлинеарности

* Стандартные ошибки коэффициентов при коррелированных регрессорах увеличиваются. Поэтому можно сделать вывод (вводящий в заблуждение), что коэффициенты незначимы.

* Невозможно оценить влияние регрессоров по отдельности.

* Кроме того, коэффициенты регрессии могут быть очень чувствительны к небольшим изменениям в данных, особенно если выборка относительно мала.

### Признаки мультиколлинеарности

Вот некоторые наиболее характерные признаки мультиколлинеарности:

* Небольшое изменение исходных данных приводит к существенному изменению оценок параметров модели.
* Оценки коэффициентов имеют малую значимость, в то время как модель в целом является значимой.
* Оценки коэффициентов имеют неправильные с точки зрения теории знаки.

### Способы обнаружения

$$
Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_m X_m + \varepsilon.
$$

Часто используемый для обнаружения мультиколлинеарности показатель называется **VIF** ("фактор инфляции вариации").

Здесь в явном виде ищется зависимость одних зависимых переменных от других.

$$
Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_m X_m + \varepsilon.
$$

Построим регрессию $X_i$ на остальные $X$ и вычислим соответствующий $R^2$:

$$
X_i = \alpha_{i0} + \alpha_1 X_1 + \cdots + \alpha_m X_m \to R_i^2.
$$

Тогда

$$
VIF(X_i) = {1 \over 1 - R_i^2}.
$$

Если какие-то VIF оказываются больше 10, то это служит признаком мультиколлинеарности.

### Методы устранения

Все методы борьбы с мультиколлинеарностью имеют свои недостатки.

Если заадчей исследования является *прогноз* будущих значений зависимой переменной, то мультиколлинеарность не является серьёзной проблемой, так как она не ухудшает значимость уравнения.

Естественным методом устранения мультиколлинеарности является *исключение отдельных переменных из модели*. Но при этом могут возникнуть новые трудности. Например, не всегда ясно, какие переменные являются "лишними". Удаление переменных может изменить содержательный смысл модели. Кроме того, коэффициент при оставшейся переменной получает смещение, т. к. теперь измеряет не только своё "чистое" влияние на $Y$, но и влияние на $Y$ всех отброшенных переменных.

## Гетероскедастичность

Условие **(D)** теоремы Гаусса-Маркова требует *постоянства дисперсии случайного члена $\varepsilon_i$* во всех наблюдениях:

> $\mathbb{D} \varepsilon_i = \sigma^2$ не зависит от $i$

Это — "гомоскедастичность".

Если же дисперсия случайного члена *меняется* от наблюдения к наблюдению, то это называется *гетероскедастичностью*.

<figure style="margin-bottom: 1rem">
    <img src="/media/images/mme_07_01.png" />
</figure>

<figure>
    <img src="/media/images/mme_07_02.png" />
</figure>

::: info Пример
Предположим, что в класс машинописи записались 100 студентов, часть из которых имеет опыт набора текста, а часть — нет.

Тогда после первого занятия будет наблюдаться большой разброс в количестве ошибок при наборе текста. После каждого последующего занятия дисперсия, вероятнее всего, будет уменьшаться. Дисперсия ошибок здесь *непостоянна* — она *уменьшается с увеличением времени*.
:::

### Последствия гетероскедастичности

Стандартные ошибки коэффициентов являются заниженными. Это приводит к завышению t-статстик и даёт неправильное (завышенное) представление о точности оценок.

### Способы обнаружения

Существует множество тестов для выявления гетероскедастичности.

#### Визуальный анализ

Один из способов выявления гетероскедастичности при помощи графического анализа состоит в том, чтобы построить диаграммы рассеяния, в каждой из которых по оси ординат стоит зависимая переменная, а по оси абсцисс — один из регрессоров.

#### Тест Уайта

Тест Уайта — это универсальный тест для обнаружения гетероскедастичности, который не требует предположений о форме зависимости диперсии ошибок от регрессоров. Основан на вспомогательной регрессии квадратов остатков на первые и вторые степени регрессоров и их попарные произведения:

$$
~ \hat{\varepsilon}_i^2 = \alpha_0 + \cdots + \alpha_i x_i + \beta_i x_i^2 + \cdots + \gamma_{ij} x_i x_j + \cdots ~
$$

```python
from statsmodels.stats.diagnostic import het_white
import statsmodels.api as sm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def heteroscedasticity_example():
    np.random.seed(42)
    n = 200

    # Создаём регрессоры
    x1 = np.random.normal(5, 2, n)
    x2 = np.random.uniform(10, 15, n)

    # Сценарий 1: сильная гетероскедастичность
    # Дисперсия ошибок зависит от x1
    error_strong = np.random.normal(0, 0.5 * np.abs(x1) + 0.1, n)
    y_strong = 2 + 3 * x1 + 2 * x2 + error_strong

    # Сценарий 2: слабая гетероскедастичность
    error_weak = np.random.normal(0, 0.1 * np.abs(x1) + 0.8, n)
    y_weak = 2 + 3 * x1 + 2 * x2 + error_weak

    # Сценарий 3: истинная гомоскедастичность
    # Дисперсия постоянна и не зависит от регрессоров
    error_homo = np.random.normal(0, 1, n)
    y_homo = 2 + 3 * x1 + 2 * x2 + error_homo

    X = sm.add_constant(np.column_stack([x1, x2]))

    print("=" * 60)

    for y, scenario in [
        (y_strong, "Сильная гетероскедастичность"),
        (y_weak, "Слабая гетероскедастичность"),
        (y_homo, "Истинная гомоскедастичность")
    ]:
        model = sm.OLS(y, X).fit()
        residuals = model.resid
    
        white_test = het_white(residuals, X)

        print(f"\n{scenario}:")
        print(f"Тест Уайта: LM = {white_test[0]:.4f}, p-value: {white_test[1]:.4f}")

        if white_test[1] < 0.05:
            print("✓ Обнаружена гетероскедастичность")
        else:
            print("✓ Гетероскедастичность не обнаружена")

heteroscedasticity_example()
```

Вывод:

```:no-line-numbers
============================================================

Сильная гетероскедастичность:
Тест Уайта: LM = 22.2886, p-value: 0.0005
✓ Обнаружена гетероскедастичность

Слабая гетероскедастичность:
Тест Уайта: LM = 5.9042, p-value: 0.3157
✓ Гетероскедастичность не обнаружена

Истинная гомоскедастичность:
Тест Уайта: LM = 4.4506, p-value: 0.4865
✓ Гетероскедастичность не обнаружена
```

#### Тест Голдфелда-Квандта

В этом тесте проверяемая гипотеза $H_0$ заключается в том, что дисперсии ошибок регрессионной модели не являются постоянными, а монотонно связаны с заранее определённой объясняющей переменной.

##### Алгоритм теста Голдфелда-Квандта

1. Наблюдения упорядочиваются по возрастанию значений этой переменной $x_i$.
2. Берутся первые $n'$ и последние $n'$ наблюдений, для них строятся регрессии, и оцениваются $ESS_1$ и $ESS_2$ соответственно.

   Число $n'$ можно взять примерно равным $n \cdot 0,4$.
3. Вычисляется

   $$
   F = {ESS_2 \over ESS_1}.
   $$

Гипотеза $H_0$ о гетероскедастичности такого вида *принимается*, если

$$
F > F_{1 - \alpha}(n' - m - 1, n' - m - 1).
$$

#### Тест Глейзера

1. Рассматривается регрессия

   $$
   |e| = a + bx^k + \varepsilon.
   $$
2. Параметр $k$ изменяется с некоторым шагом, и для каждого его значения строится регрессия.
3. *Значимость* уравнения при каком-либо значении $k$ означает, что *гипотеза о гетероскедастичности не отклоняется*.

### Методы устранения

Гетероскедастичность означает, что дисперсии остатков не одинаковые, а разные для разных наблюдений.

Это создаёт проблему: если дисперсии не постоянны, то относительная важность каждого наблюдения не одинакова.

Чем больше дисперсия, тем меньшее значение (или вес) должно придаваться данному наблюдению.

## Автокорреляция

Условие **(E)** теоремы Гаусса-Маркова требует отсутствия корреляции между значениями случайного фактора $\varepsilon_i$:

> $cov(\varepsilon_i \varepsilon_j) = 0$ при $i \ne j$

Нарушение этого условия, т. е. наличие связи между случайными численами, называется *автокорреляцией*.

Обычно автокорреляция рассматривается при изучении временных рядов (т. е. когда номер наблюдения соответствует моменту времени).

::: info Пример
Авторегрессия первого порядка AR(1):

$$
\varepsilon_i = \rho \cdot \varepsilon_{i - 1} + u_i, ~~~ -1 < \rho < 1,
$$

где

* $\varepsilon$ — случайный член уравнения регрессии,
* $\rho$ — коэффициент авторегрессии,
* $u$ — случайный член, не подверженный автокорреляции.

В этом случае $\varepsilon$ в *данном* наблюдении прямо связан лишь с $\varepsilon$ в *предыдущем* наблюдении.

Если $\rho > 0$, то автокорреляция положительная, если $\rho < 0$, то отрицательная.
:::

::: info Пример
Нулевая автокорреляция:

<figure>
    <img src="/media/images/mme_07_03.png" />
</figure>

$$
\varepsilon_ = 0 \cdot \varepsilon_{i-1} + u_i.
$$
:::

::: info Пример
Положительная автокорреляция:

<figure>
    <img src="/media/images/mme_07_04.png" />
</figure>

$$
\varepsilon_ = 0,5 \cdot \varepsilon_{i-1} + u_i.
$$
:::

::: info Пример
Очень сильная положительная автокорреляция:

<figure>
    <img src="/media/images/mme_07_05.png" />
</figure>

$$
\varepsilon_ = 0,9 \cdot \varepsilon_{i-1} + u_i.
$$
:::

::: info Пример
Сильная отрицательная автокорреляция:

<figure>
    <img src="/media/images/mme_07_06.png" />
</figure>

$$
\varepsilon_ = -0,8 \cdot \varepsilon_{i-1} + u_i.
$$
:::

### Способы обнаружения

Существуют различные способы выявления автокорреляции.

#### Критерий Дарбина-Уотсона

Этот метод применяется для обнаружения авторегресии AR(1):

$$
\varepsilon_i = \rho \cdot \varepsilon_{i-1} + u_i.
$$

В критерии Дарбина-Уотсона проверяется гипотеза $H_0: ~ \rho = 0$ при альтернативной $\rho \ne 0$.

При этом используется статистика Дарбина-Уотсона:

$$
DW = {
    \sum (e_i - e_{i-1})^2 \over \sum e_i^2
}, ~~~ e_i = y_i - \hat{y}_i.
$$

<figure>
    <img src="/media/images/mme_07_07.png" />
</figure>

*($d_L$ и $d_U$ — табличные значения.)*

:::: info Пример
Построена регрессия с $n = 8$, $m = 2$. Используя тест Дарбина-Уотсона, проверить гипотезу о наличии авторегрессии первого порядка при $\alpha = 0,05$.

| $y$ | $\hat{y}$ |
|-|-|
| 3 | 3,09 |
| 5 | 4,68 |
| 5 | 5,79 |
| 6 | 6,13 |
| 7 | 8,43 |
| 9 | 7,39 |
| 9 | 10,02 |
| 11 | 9,46 |

::: info Решение
Заполним таблицу:

| $e_i = y_i - \hat{y}_i$ | $e_{i-1}$ | $(e_i - e_{i-1})^2$ | $e_i^2$ |
|-|-|-|-|
| -0,09 | — | — | 0,0081 |
| -0,32 | -0,09 | 0,17 | 0,1024 |
| -0,79 | -0,32 | 1,23 | 0,6241 |
| -0,13 | -0,79 | 0,44 | 0,0169 |
| -1,43 | -0,13 | 1,69 | 2,0449 |
| 1,61 | -1,43 | 9,24 | 2,5921 |
| -1,02 | 1,61 | 6,92 | 1,0404 |
| 1,54 | -1,02 | 6,55 | 2,3716 |
||||
| $\boldsymbol{\Sigma}$ | | **= 26,2** | **= 8,8** |

$$
DW = {26,2 \over 8,8} \approx 3.
$$

По таблице $d_L = 0,36$, $d_U = 1,78$.

Число $DW = 3$ попадает в зону $(4 - d_U; 4-d_L) = (2,22; 3,64)$, поэтому критерий Дарбина-Уотсона в этом примере ответа не даёт.
:::
::::