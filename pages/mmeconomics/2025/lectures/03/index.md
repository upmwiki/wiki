---
prev:
    text: Эконометрические модели
    link: ../02/
next:
    text: Метод наименьших квадратов
    link: ../04/
---

**Математические модели в экономике**

# Корреляционный анализ количественных зависимостей

<p class="subtext">Лекция</p>

В прикладных задачах часто нужно понять: связаны ли две случайные величины?

Примеры:

* часы подготовки к экзамену $\leftrightarrow$ итоговый балл;
* температура воздуха $\leftrightarrow$ потребление электроэнергии;
* доход $\leftrightarrow$ расходы.

**Корреляционный анализ** — это инструментарий для измерения тесноты и направления связи между переменными.

## Коэффициент корреляции Пирсона

При изучении связи между факторами сразу возникает вопрос: в какой степени две переменные *совместно изменяются*? (т. е., влечёт ли за собой увеличение одной переменной увеличение или уменьшение другой, или не влечёт.)

<figure>
    <img width="60%" src="/media/images/mmeconomics_1.png" />
</figure>

Представляет интерес наклон (**направление связи**) и ширина (**сила связи**) воображаемого эллипса.

Тесноту *линейной* связи между двумя факторами обычно измеряют при помощи **коэффициента корреляции Пирсона:**

$$
r^*_{XY} = {\overline{xy} - \bar{x} \cdot \bar{y} \over \sqrt{D^*_x} \cdot \sqrt{D^*_y}}
$$

Известно, что

$$
-1 \le r^*_{XY} \le 1
$$

и *линейная зависимость между $X$ и $Y$* тем сильнее, чем ближе *модуль коэффициента корреляции* к 1.

::: info Пример

| $X/Y$ | $-1$ | $0$ | $2$ |
|-|-|-|-|
| $0-20$ | $5$ | $3$ | $4$ |
| $20-40$ | $2$ | $1$ | $5$ |

$$
\bar{x} = {10 \cdot 12 + 30 \cdot 8 \over 20} = {360 \over 20} = 18;
$$

$$
\bar{y} = {-1 \cdot 7 + 0 \cdot 4 + 2 \cdot 9 \over 20} = {11 \over 20} = 0,55;
$$

$$
\overline{x^2} = {10^2 \cdot12 + 30^2 \cdot 8 \over 20} = {8400 \over 20} = 420;
$$

$$
D^*_x = \overline{x^2} - (\bar{x})^2 = 420 - 18^2 = 96;
$$

$$
\overline{y^2} = {(-1)^2 \cdot 7 + 0^2 \cdot 4 + 2^2 \cdot 9 \over 20} = {43 \over 20} = 2,15;
$$

$$
D^*_y = \overline{y^2} - (\bar{y})^2 = 2,15 - 0,55^2 = 1,8475;
$$

$$
\overline{xy} = {10 \cdot (-1) \cdot 5 + 10 \cdot 3 \cdot 0 + 10 \cdot 4 \cdot 2 + 30 \cdot 2 \cdot (-1) + 30 \cdot 1 \cdot 0 + 30 \cdot 5 \cdot 2 \over 20} =
$$

$$
= {270 \over 20} = 13,5;
$$

$$
r^*_{XY}
= {\overline{xy} - \bar{x} \cdot \bar{y} \over \sqrt{D^*_x} \sqrt{D^*_y}}
= {13,5 - 18 \cdot 0,55 \over \sqrt{96} \sqrt{1,8475}}
\approx {3,6 \over 9,79 \cdot 1,36 }
\approx 0,27.
$$
:::

### Почему именно такая формула?

**Ковариация** $Cov(X,Y)$ — это мера совместного отклонения двух переменных от своих средних:

$$
Cov(X,Y) = \overline{(X - \bar{X})(Y - \bar{Y})} = \overline{XY} - \bar{X} \cdot \bar{Y}.
$$

Если $X$ и $Y$ одновременно выше или ниже среднего — ковариация положительна. Если одна выше, другая ниже — отрицательна.

Но ковариация *зависит от масштаба*. Например, если измерять рост в метрах или сантиметрах — ковариация изменится в 100 раз. Чтобы сделать меру безразмерной и сравнимой. делим ковариацию на произведение стандартных отклонений:

$$
r = {Cov(X,Y) \over \sqrt{D^*_x} \sqrt{D^*_y}}.
$$

Теперь:

* $r \in [-1; 1]$;
* не зависит от единиц измерения;
* интерпретируем: близость к $\pm 1$ — сильная линейная связь.

### Геометрическая интерпретация

Представим выборку как векторы в $\mathbb{R}^n$:

$$
x = (x_1, x_2, ..., x_n),
$$

$$
y = (y_1, y_2, ..., y_n).
$$

Центрируем их: вычитаем среднее — получаем векторы

$$
x_c = (x_1 - \bar{x}, x_2 - \bar{x}, ..., x_n - \bar{x}),
$$

$$
y_c = (y_1 - \bar{y}, y_2 - \bar{y}, ..., y_n - \bar{y}).
$$

Тогда

$$
r = {x_c \cdot y_c \over ||x_c|| \cdot ||y_c||} = \cos \theta,
$$

где $\theta$ — угол между векторами.

Таким образом, коэффициент корреляции — это косинус угла между центрированными векторами данных.

* Если $r = 1$, то векторы сонаправлены (идеальная положительная линейная связь).
* Если $r = -1$, то противоположно направлены.
* Если $r = 0$, то векторы ортогональны (нет линейной связи).

Это объясняет, почему корреляция измеряет линейную связь: она отражает, до какой степени данные близки к прямой.

---

Корреляцию лучше всего использовать для переменных, которые демонстрируют приближённо линейную зависимость между собой. Соответствие данных может быть визуально представлено в виде диаграммы рассеяния. При помощи диаграммы рассеяния мы можем оценить связь между переменными и определить, коррелируют они или нет.

<figure>
    <img width="75%" src="/media/images/mmeconomics_2.png" />
</figure>

Вот некоторые диаграммы рассеивания вместе с выборочным коэффициентом корреляции:

<figure>
    <img width="75%" src="/media/images/mmeconomics_3.png" />
</figure>

::: info Пример
Джон — инвестор. Его портфель в основном отслеживает показатели S&P 500, и Джон хочет добавить акции Apple Inc. Прежде чем добавить Apple в свой портфель, он хочет оценить корреляцию между акциями и S&P 500, чтобы убедиться, что добавление акций не увеличит систематический риск его портфеля. Чтобы найти коэффициент, Джон собрал информацию за пять лет:

| | S&P 500 | Apple |
|-|-|-|
| 2013 | 1691.75 | 68.96 |
| 2014 | 1977.80 | 100.11 |
| 2015 | 1884.09 | 109.06 |
| 2016 | 2151.13 | 112.18 |
| 2017 | 2519.36 | 154.12 |

Рассчитанный по двум последним столбцам коэффициент $r = 0,95$ показывает, что цены S&P 500 и Apple Inc. имеют высокую положительную корреляцию. Это означает, что их соответствующие цены имеют тенденцию двигаться в одном направлении. Поэтому добавление Apple в портфель фактически увеличит уровень систематического риска.
:::

### Замечания о коэффициенте корреляции

1. Коэффициент корреляции оценивает только *линейную* связь переменных. Он не покажет наличие нелинейной связи.

   <figure>
       <img width="50%" src="/media/images/mmeconomics_4.png" />
       <figcaption>Здесь связь переменных есть, и она очень сильная, но <i>r<sub>XY</sub></i> = 0.</figcaption>
   </figure>

2. Необходимо, чтобы у переменных была значительная изменчивость. Если сформулировать выборку изначально однотипных по одному из рассматриваемых факторов, нечего надеяться выявить там кореляцию фактора с другими.

   <figure>
       <img width="65%" src="/media/images/mmeconomics_5.png" />
   </figure>

3. Коэффициент корреляции очень чувствителен к выбросам.

::: tip Важное замечание
Корреляция совершенно не подразумевает наличие причинно-следственной связи.

Если две переменные коррелируют, это не означает, что одна переменная вызывает изменения в другой переменной. Могут существовать другие факторы, обуславливающие эти взаимосвязи.

Зная только коэффициент корреляции, нельзя сколько-нибудь детально описать зависимость.

<figure>
    <img width="80%" src="/media/images/mmeconomics_6.png" />
</figure>
:::

### Матрица корреляций

:::: info Пример
Для следующих данных составить корреляционную матрицу:

| $y$ | $x_1$ | $x_2$ |
|-|-|-|
| $7$ | $2$ | $3$ |
| $14$ | $2$ | $4$ |
| $11$ | $5$ | $7$ |
| $12$ | $7$ | $10$ |

::: info Решение
1. Составляем матрицу:

   $$
   M = \begin{pmatrix}
    7 & 2 & 3 \\
    14 & 2 & 4 \\
    11 & 5 & 7 \\
    12 & 7 & 10
   \end{pmatrix}.
   $$

2. Из каждого столбца вычитаем его среднее значение (например, из второго столбца вычитаем $4$):

   $$
   M_1 = \begin{pmatrix}
    -4 & -2 & -3 \\
    3 & -2 & -2 \\
    0 & 1 & 1 \\
    1 & 3 & 4
   \end{pmatrix}.
   $$

3. Умножаем $M_1^T$ на $M_1$ и делим на $n$ (здесь $n = 4$), получаем **матрицу ковариаций:**
   
   $$
   M_1^T \cdot M_1
   = \begin{pmatrix}
    -4 & 3 & 0 & 1 \\
    -2 & -2 & 1 & 3 \\
    -3 & -2 & 1 & 4
   \end{pmatrix} \cdot \begin{pmatrix}
    -4 & -2 & -3 \\
    3 & -2 & -2 \\
    0 & 1 & 1 \\
    1 & 3 & 4
   \end{pmatrix} / 4
   = \begin{pmatrix}
    6,5 & 1,25 & 2,5 \\
    1,25 & 4,5 & 5,75 \\
    2,5 & 5,75 & 7,5
   \end{pmatrix}.
   $$

   На диагонали стоят *выборочные дисперсии*, например, $\mathbb{D}y = 6,5$.

4. Делим каждое число на корень из произведения соответствующих дисперсий(например, $1,25$ на корень из $6,5 \cdot 4,5$; $2,5$ на корень из $6,5 \cdot 7,5$), получаем **матрицу корреляций:**
   
   $$
   r \approx \begin{pmatrix}
    1 & 0,2311 & 0,3581 \\
    0,2311 & 1 & 0,9898 \\
    0,3581 & 0,9898 & 1
   \end{pmatrix}.
   $$
:::
::::

В MS Excel для вычисления корреляционных матриц используется инструмент *Корреляция* из надстройки *Анализ данных*.

<!-- <figure>
    <img width="50%" src="/media/images/mmeconomics_7.png" />
</figure> -->

1. Вызвать надстройку *Сервис* (или вкладка *Данные*) — *Анализ данных*.
2. В появившемся списке *Инструменты анализа* выбрать строку *Корреляция* и нажать кнопку ОК.
3. В появившемся окне укзаать *Входной интервал*.
4. Установить переключатель в разделе *Группировка* ("по столбцам" или "по строкам").

В выходной диапазон будет выведена корреляционная матрица.

### Частная корреляция

Корреляция между двумя переменными, вычисленная после устранения всех других переменных, называется **частной корреляцией**.

*Частный коэффициент корреляции* между переменными $X$ и $Y$ при исключении влияния переменной $Z$ вычисляется по формуле

$$
r_{XY|Z} = {r_{XY} - r_{XZ} r_{YZ} \over \sqrt{
    \left( 1 - r_{XZ}^2 \right) \left( 1 - r_{YZ}^2 \right)
}}.
$$

#### Откуда берётся эта формула?

1. **Идея — "очистить" переменные от влияния $Z$.** Чтобы измерить чистую связь между $X$ и $Y$, нужно:

   1. Убрать из $X$ ту часть, которая объясняется $Z$, следовательно, получить остатки регресии $X$ на $X$.
   2. То же самое — для $Y$, следовательно, получить остатки регрессии $Y$ на $Z$.
   3. Найти корреляцию между этими остатками.

   Остатки представляют ту часть $X$ и $Y$, которая не зависит от $Z$.

2. **Регрессионная интерпретация.** Рассмотрим линейные модели:

   $$
   X = a + b_Z Z + \varepsilon_X,
   ~~~~
   Y = c + d_Z Z + \varepsilon_Y.
   $$

   Остатки:

   $$
   e_X = X - \hat{X},
   ~~~~
   e_Y = Y - \hat{Y}.
   $$

   Тогда

   $$
   r_{XY|Z} = \text{Corr}(e_X, e_Y).
   $$

   Это определение частной корреляции через остатки регрессии.

3. **Геометрическая интерпретация.** В $n$-мерном пространстве наблюдений:

   * векторы $X$, $Y$, $Z$ — это точки в $\mathbb{R}^n$;
   * регрессия $X$ на $Z$ — это проекция вектора $X$ на направление $Z$;
   * остаток $e_X$ — это перпендикуляр от $X$ до прямой, натянутой на $Z$.

   Тогда $r_{XY|Z}$ — это косинус угла между остатками $e_X$ и $e_Y$. Отсюда прямым вычислением и получается формула частной корреляции.

::: info Пример
Пусть
* $X$ — продажи мороженого,
* $Y$ — число утоплений,
* $Z$ — температура воздуха.

Известно, что
* $r_{XY} = 0,8$ — сильная положительная корреляция,
* $r_{XZ} = 0,9$, $r_{YZ} = 0,85$.

Тогда

$$
r_{XY|Z} = {
    0,8 - 0,9 \cdot 0,85
    \over
    \sqrt{ (1 - 0,9^2) (1 - 0,85^2) }
} \approx 0,15.
$$

Следовательно, после учёта температуры связь почти исчезает! Значит, первоначальная корреляция была ложной — обе переменные зависели от третьей (температуры).
:::

::: info Пример
Из рисунка видно, что доход положительно связан с ростом, весом, образованием и afdp (процентный балл по квалификационному тесту вооружённых сил); отрицательно — с семейным положением (холост = 1, женат = 2, разошёлся = 3, разведён = 4, вдовец = 5), полом (мужчина = 1, женщина = 2).

<figure>
    <img src="/media/images/mme_03_01.png" />
</figure>

Эта отрицательная корреляция означает, что женщины зарабатывают немного меньше.

Данные в некоторой степени подтверждают то, что мы уже знаем, однако одна вещь довольно подозрительна — высокие люди зарабатывают больше. На самом деле, эта корреляция является ложной. Посмотрим частную корреляцию.

<figure>
    <img width="60%" src="/media/images/mme_03_01.png" />
    <figcaption>Корреляция</figcaption>
</figure>

<figure>
    <img width="60%" src="/media/images/mme_03_02.png" />
    <figcaption>Частная корреляция</figcaption>
</figure>
:::

::: info Пример
Пусть есть матрица корреляций:
$$
r = \begin{pmatrix}
    1 & 0,2311 & 0,3581 \\
    0,2311 & 1 & 0,9898 \\
    0,3581 & 0,9898 & 1
\end{pmatrix}
$$

Вычислим частные производные.

Вычислим $r_{yx_1 | x_2}$, $r_{yx_2 | x_1}$:

$$
r_{yx_1 | x_2} = {
    0,2311 - 0,3581 \cdo 0,9898
    \over
    \sqrt{ (1 - 0,3581^2) (1 - 0,9898^2) }
} = -0,925;
$$

$$
r_{yx_2 | x_1} = {
    0,3581 - 0,2311 \cdot 0,9898
    \over
    \sqrt{ (1 - 0,2311^2) (1 - 0,9898^2) }
} = 0,9311.
$$

И так далее...

В итоге получим:

$$
r^\text{частн} =
\begin{pmatrix}
    1 & -0,925 & 0,9311 \\
    -0,925 & 1 & 0,9984 \\
    0,9311 & 0,9984 & 1
\end{pmatrix}
$$
:::

#### Связь корреляции Пирсона с нормальным законом

Для двумерного нормального распределения ситуация особенная.

Если вектор $(X, Y)$ имеет двумерное нормальное распределение, то *нулевая корреляция* $\iff$ *независимость*. Это исключительное свойство нормального распределения!

В общем случае: $\text{Cov}(X, Y) = 0$ не означает независимости.

Более того, полное совместное распределение $(X, Y)$ однозначно определяется: математическими ожиданиями $\mu_X$, $\mu_Y$, дисперсиями $\sigma_X^2$, $\sigma_Y^2$ и коэффициентом корреляции $\rho$. То есть, в случае нормальности вся информация о зависимости содержится в $\rho$.

**Линейность условного математического ожидания.** Если $(X, Y) \sim N(\mu, \Sigma)$, то:

$$
M [Y | X = x] = \mu_Y + \rho \cdot {\sigma_X \over \sigma Y} \cdot (x - \mu_X).
$$

То есть условное среднее — линейная функция.

Это означает, что:
* оптимальный предиктор $Y$ по $X$ — линейная регрессия;
* коэффициент корреляции напрямую связан с наклоном этой прямой.

Таким образом, в нормальном случае корреляция не просто мера связи — она определяет оптимальную линейную модель прогноза.

**Оценки корреляции наиболее эффективны при нормальности.** В статистике:

* оценка $r$ (выборочный коэффициент) является несмещённой и состоятельной оценкой $\rho$ в общем случае;
* её дисперсия минимальна, когда $(X, Y)$ нормальны;
* при нормальности можно построить доверительные интервалы и проверять гипотезы с помощью точных распределений.

**А если распределение не нормальное?** Тогда:

* $r$ всё ещё измеряет линейную связь, но может быть *неинформативным*;
* может возникнуть ложная корреляция из-за выбросов;
* *нулевая корреляция не означает независимости;*
* оптимальный прогноз $Y$ по $X$ может быть *нелинейным*, а $r$ этого не отразит.

В таких случаях лучше использовать:
* ранговые коэффициенты (Спирмен, Кенделл) — устойчивы к выбросам и не требуют нормальности;
* непараметрические методы — для нелинейных зависимостей.

## Коэффициент ранговой корреляции Спирмена

Коэффициент Спирмена — это *непараметрическая* мера корреляции, которая оценивает силу и направление монотонной (не обязательно линейной) связи между двумя переменными. Он основан на ранжировании данных, а не на их абсолютных значениях.

Это делает его полезным для *порядковых* данных или когда распределение переменных *не нормальное*, есть выбросы или данные *нелинейны*, но сохраняют *монотонность* (т. е. когда одна переменная растёт, вторая тоже растёт или падает последовательно).

### Формула расчёта

Для двух наборов данных $X$ и $Y$ с $n$ наблюдениями:

1. Присвойте ранги $R_i$ и $S_i$ каждому значению $X$ и $Y$ (ранг $1$ — наименьшему, ранг $n$ — наибольшему). Если есть одинаковые значения (связи), присвойте средний ранг (например, два одинаковых значения на 3-м и 4-м месте получают ранг 3,5).
2. Вычислите разницы рангов $d_i = \text{ранг}_{X_i} - \text{ранг}_{Y_i}$ для каждого $i$.
3. Используйте формулу:

   $$
   \rho = 1 - {6 \over n \left(n^2 - 1\right)} \sum_{i=1}^n d_i^2.
   $$

Эта формула работает для данных без связей (совпадений) или с небольшим их количеством.

Для данных с многими связями используется корректированная версия, но для простоты нередко применяют базовую.

Если данные уже ранжированы (например, оценки от 1 до 5), то коэффициент Спирмена эквивалентен коэффициенту Пирсона, применённому к рангам.

::: info Пример 1 (простой расчёт без связей)
Даны ранги двух судей на конкурсе талантов для 5 участников:
* судья A (ранги): 1, 2, 3, 4, 5;
* судья B (ранги): 2, 1, 4, 3, 5.

Разности рангов:

$$
d_i: ~ ~
(1 - 2) = -1;
~
(2 - 1) = 1;
~
(3 - 4) = -1;
~
(4 - 3) = 1;
~
(5 - 5) = 0.
$$

Далее:

$$
\sum d_i^2 = 1 + 1 + 1 + 0 = 4;
$$

$$
n = 5;
$$

$$
\rho = 1 - {6 \cdot 4 \over 5 \cdot (25 - 1)} = 1 - {24 \over 5 \cdot 24} = 0,8.
$$

Сильная положительная корреляция ($\rho = 0,8$). Судьи в основном согласны, но есть и небольшие расхождения.

#### Отличия от коэффициента корреляции Пирсона

Оба измеряют корреляцию от -1 до +1, но подходы разные. Пирсон — параметрический метод, Спирмен — непараметрический.

1. **Тип связи:**

   * *Пирсон:* оценивает *линейную* корреляцию. Предполагает, что связь между переменными приближённо линейная. Если связь существенно нелинейная, $r$ может быть низким, даже если связь сильная.
   * *Спирмен:* оценивает *монотонную* корреляцию (возрастающую или убывающую, но не обязательно линейную). Подходит для кривых зависимостей, если они монотонны (например, экспоненциальный рост).

2. **Данные и предположения:**

   * *Пирсон:* работает с *интервальными или количественными данными* (как рост, вес). Предполагает нормальное распределение переменных, линейность, отсутствие выбросов и гомоскедастичность (равномерную дисперсию).
   * *Спирмен:* работает с *порядковыми или ранжированными данными*. Не требует нормальности, устойчив к выбросам (ранжирование "сглаживает" экстремальные значения). Можно применять к количественным данным, преобразовав их в ранги.

3. **Формула и расчёт:**

   * *Спирмен:* по сути, это Пирсон, применённый к рангам данных.

4. **Чувствительность к выбросам и распределению:**

   * *Пирсон:* чувствителен к выбросам — один экстремальный пункт может сильно исказить $r$.
   * *Спирмен:* устойчив к выбросам, так как ранжирование минимизирует их влияние.

5. **Применение и интерпретация:**

   * *Пирсон:* идеален для данных с нормальным распределением. Интерпретируется как доля объяснённой дисперсии ($r^2$).
   * *Спирмен:* для социальных наук, где данные субъективны или не нормальны (опросы, рейтинги). Менее мощный статистически (меньше чувствителен к слабым связям), но более робастный.
:::

## Коэффициент ранговой корреляции Кэндалла

Это ещё один непараметрический метод для оценки силы и направления связи между двумя переменными на основе их рангов.

Коэффициент Кендалла измеряет степень согласованности (concordance) между двумя наборами рангов. Он фокусируется на парах наблюдений: сколько пар "согласованных" (concordant, когда порядок рангов совпадает) и сколько "несогласованных" (discordant, когда порядок обратный).

Это делает его полезным для порядковых данных или когда мы хотим оценить монотонную связь без предположения о линейности.

### Формула расчёта

Для двух наборов данных $X$ и $Y$ с $n$ наблюдениями:

1. Присвойте ранги значениям $X$ и $Y$ (как в Спирмене: средний ранг для связей).
2. Рассмотрите все возможные пары $(i, j)$, где $i < j$.
3. Каждая пара относится к одному из типов:
   * *Согласованы:* если есть совпадение:
     $$
     \text{ранг}_{X_i} < \text{ранг}_{X_j} \text{ и } \text{ранг}_{Y_i} < \text{ранг}_{Y_j}
     $$

     либо
     
     $$
     (\text{ранг}_{X_i} > \text{ранг}_{X_j} \text{ и } \text{ранг}_{Y_i} > \text{ранг}_{Y_j}.
     $$
   * *Не согласованы:* если неравенства в противоположных направлениях.
   * *"Связь":* если равны ранги в $X$ или $Y$.

Формула для $\tau_b$:

$$
\tau_b = {
    S - N
    \over
    \sqrt{ (S + N + T_X) (S + N + Y_Y) }
},
$$

где

* $S$ — число согласованных пар,
* $N$ — число несогласованных пар,
* $T_X = \sum (t_i (t_i - 1) / 2)$ для каждой группы в $X$ размером $t_i$;
* $T_Y$ — аналогично для $Y$.

Общее число пар: $n(n-1) / 2$.

Без связей формула упрощается до:

$$
\tau = {
    2 (S - N)
    \over
    n(n-1)
}.
$$

::: info Пример
Данные:
* $X: ~ 1, 2, 2, 4$ (ранги: 1, 2.5, 2.5, 4);
* $Y: ~ 1, 3, 2, 4$ (ранги: 1, 3, 2, 4).

Расчёт пар: $S = 5$, $N = 0$, $T_X = 2 \cdot (2 - 1) = 2$, $T_Y = 0$,

$$
\tau_b = {
    5 - 0
    \over
    \sqrt{ (5 + 0 + 1) (5 + 0 + 0) }
} = {5 \over \sqrt{30}} \approx 0,9128.
$$

<!-- Интерпретация: умеренная корреляция. -->
:::

### Сравнение с коэффициентом корреляции Спирмена

1. **Подход к измерению связи:**

   * *Спирмен:* основан на разницах рангов:
     $$
     \rho = 1 - 6 \sum {d^2 \over n(n^2 - 1)}.
     $$
     Это, по сути, коэффициент Пирсона, применённый к рангам — измеряет, насколько ранги линейно связаны.
   * *Кендалл:* основан на подсчёте concordant и discordant пар. Измеряет вероятность согласованности пар, что делает его более "глобальным" и веряотностным.

2. **Обработка связей:**

   * *Спирмен:* простая корректировка — средние ранги, но формула не меняется радикально. Много совпадений могут искажать (используется поправка в ПО).
   * *Кендалл:* более робастен — $\tau_b$ явно учитывает совпадения в знаменателе, минимизируя их влияние. Лучше для данных с многими повторениями (например, опросы с Likert-шкалой).

3. **Вычислительная сложность и точность:**

   * *Спирмен:* $O(n \log n)$ (из-за сортировки для ранжирования), проще в расчёте вручную для малых $n$.
   * *Кендалл:* $O(n^2)$ (проверка всех пар), сложнее вручную, но точнее для малых выборок ($n < 10$), где даёт более консервативные оценки.

4. **Значения и чувствительность:**

   * *Спирмен:* обычно даёт большие абсолютные значения ($|\rho| > |\tau|$) для тех же данных). Более чувствителен к измерениям в середине распределения.
   * *Кендалл:* консервативнее, значения меньше (пример: для идеальной корреляции без совпадений оба = 1, но с расхождениями $\tau$ ниже). Лучше выявляет "истинную" согласованность, менее подвержен влиянию экстремальных рангов.

5. **Применение и интерпретация:**

   * *Спирмен:* универсален для порядковых данных, хорош для больших $n$, часто используется в психометрике.
   * *Кендалл:* предпочтителен в анализе согласованности (например, межэкспертная надёжность), малых выборках или с совпадениями. В статистических пакетах (R, Python) оба доступны, но Кендалл чаще в продвинутом анализе.

::: info Пример сравнения на данных
Возьмём данные из примера 1 выше:
$$
X: ~ 1,2,3,4, ~~~ Y: ~ 1,3,2,4.
$$

* Спирмен:
  $$
  d_i: ~ 0, -1, 1, 0;
  ~~~
  \sum d_i^2 = 0 + 1 + 1 + 0 = 2;
  $$

  $$
  \rho = 1 - {62 \over 415} = 1 - {12 \over 60} = 0,8.
  $$

* Кендалл: $\tau \approx 0,667$ (как выше).

Интерпретация: оба положительные, но Спирмен выше — он "прощает" небольшие расхождения сильнее. Если добавить ties, разница вырастет.
:::

Код на Python:

```python
import numpy as np
from scipy import stats

# Данные
x = np.array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ])
y = np.array([ 5, 4, 6, 7, 7, 8, 9, 9, 10, 10 ])

# Пирсон
r_pearson, p_pearson = stats.pearsonr(x, y)

# Спирмен
rho_spearman, p_spearman = stats.spearmanr(x, y)

# Кендалл
tau_kendall, p_kendall = stats.kendalltau(x, y)

print("Пирсон:", r_pearson, "p=", p_pearson)
print("Спирмен:", rho_spearman, "p=", p_spearman)
print("Кендалл:", tau_kendall, "p=", p_kendall)
```

Вывод:

```:no-line-numbers
Пирсон: 0.9670282541035465 p= 4.968872857109241e-06
Спирмен: 0.9786344578900984 p= 8.884953987050218e-07
Кендалл: 0.9200874124564723 p= 0.0002929792623413827
```